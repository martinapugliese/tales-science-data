{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, config_ipython\n",
    "config_ipython()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is in a nutshell\n",
    "\n",
    "Also called *Latent Semantic Indexing*, it is a technique first described in [[1]](#1) and then in [[2]](#2). It is a method which uses SVD to identify patterns in collections of texts encoded numerically in a matrix. The bulk of the idea is that words used in the same context have similar meanings, so that semantically related terms show latent correlations. In the context of Information Retrieval, this allows a query against documents which have undergone LSA to output results which are conceptually similar in meaning to the query even if the words are not the same, so this works as an improvement of a simple keyword search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work\n",
    "\n",
    "* The first thing is to build the term-document matrix $M$, an $n \\times m$ matrix ($n$ number of documents in collection, $m$ number of unique terms)\n",
    "* Then we weigh the data in the matrix so that each cell value $m_{ij}$ becomes a multiplication of a local weight $l_{ij}$ (a function of the relative frequency $f_{ij}$ of the term $i$ in document $j$) and a global weight $g_i$ (the relative frequency of term $i$ in the entire collection of documents)\n",
    "\n",
    "    Now, for the *local weight*, typical choices of the function of the occurrences are\n",
    "\n",
    "    * *binary*: \n",
    "    $$\n",
    "    l_{ij} = \n",
    "    \\begin{cases}\n",
    "        1 \\text{ if term } i \\text{exists in doc} j\\\\\n",
    "        0 \\text{ else}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    * *term frequency*: the actual number of occurrences $f_{ij}$ of term $i$ in doc $j$\n",
    "    * *log*: $l_{ij} = \\log(f_{ij} + 1)$\n",
    "    * *augnorm*: $\\frac{\\frac{f_{ij}}{\\max_{ij} f_{ij}}}{2}$\n",
    "\n",
    "   For the *global weight* instead, functions can be\n",
    "\n",
    "    * *binary*: $g_i = 1$\n",
    "    * *normal*: $g_i = \\frac{1}{\\sqrt{\\sum_j f_{ij}^2}}$\n",
    "    * *IDF* (see [reference](../concepts/text-num-feats.ipynb#The-TF-IDF-framework)): $g_i = \\log_2{\\frac{n}{1 + d_i}}$, with $d_i$ is the number of documents in which term $i$ appears and $N$ being the number of documents in the collection\n",
    "    * *GF-IDF* : $g_i = \\frac{f^g_i}{d_i}$, where $f^g_i$ is the total number of times term $i$ appears in the whole collection ($g$ stands for \"global\"); $d_i$ is the number of documents in which term appears\n",
    "    * *entropy*: $g_i = 1 + \\frac{\\sum_j p_{ij} \\log{p_{ij}}}{\\log n}$, with $p_{ij} = \\frac{f_{ij}}{f^g_i}$\n",
    "\n",
    "* An [SVD](../../maths/dim-reduction/svd.ipynb) is run on the matrix so it is decomposed into three matrices as $M  = T S D^t$, $T$ being an $m \\times r$ term-concept matrix, $S$ that of singular values ($r \\times r$), $D$ the concept-document matrix ($n \\times r$), which respect $T T^1 = \\mathbb{1}$; $D^t D = \\mathbb{1}$ and $s_{11} \\geq s_{22} \\geq \\ldots \\geq s_{rr} > 0$, $s_{ij} = 0$ when $i \\neq j$.\n",
    "\n",
    "\n",
    "* The SVD is truncated to reduce the rank and keep only the largest $k \\ll r$ singular values, so that the dimensionality is effectively reduced to $k$ and only the most important semantic information is kept. Actually, the efficient LSI algorithms compute directly these $k$ singular values instead of the SVD of the full matrix and then truncate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then ...\n",
    "\n",
    "The reduced $T$, $D$, $S$ will define the new vector spaces and embody the conceptual information in the collection. The similarity of terms or documents is computed in these spaces: for instance, the similarity between documents $j$ and $l$ will be computed as the similarity (typically cosine) between the corresponding vectors in the document space; same for the similarity between terms. \n",
    "\n",
    "A query $q$ can be transformed into this space as well, as $\\hat q = S^{-1} T q$, and then similarity to documents can be used to retrieve the best matching ones. \n",
    "\n",
    "Note that \n",
    "\n",
    "$$\n",
    "M = T S D^t \\iff M^t T S^{-1} = D \\ ,\n",
    "$$\n",
    "\n",
    "so a new vector for a new document can be calculated by multiplying a new column in $M^t$, but this is only a good idea if the new document contains terms and concepts which are already represented, otherwise they'd be ignored: to account for them the SVD has to be recalculated. The process of augmenting the LSI with new documents is called *folding-in*. \n",
    "\n",
    "### Applications\n",
    "\n",
    "The low-dimensional space can be used to\n",
    "\n",
    "* compare documents (clustering, document classification, ...)\n",
    "* find relations between terms (synonymy, ...)\n",
    "* find similar documents across languages\n",
    "\n",
    "### Note that\n",
    "\n",
    "* A PCA approach on these problems would not work as it would require a normalisation of the matrix, which would imply a loss of variability in the lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â References\n",
    "\n",
    "1. <a name=\"1\"></a> S Deerwester,  [**Indexing by Latent Semantic Analysis**]() *Journal of the American society for information science*, 41, 1990\n",
    "2. <a name=\"1\"></a> C H Papadimitriou, P Raghavan, H Tamaki, S Vempala, [**Latent Semantic Indexing: a Probabilistic Analysis**](https://pdfs.semanticscholar.org/6406/70d83e83427ff85ce2fbe4381d517f9512c1.pdf), *Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems ACM*, 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
