{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, config_ipython\n",
    "config_ipython()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is and how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLSA, also called PLSI (I for \"indexing\"), is a probabilistic version of LSA which models the co-occurrence of words and documents as a mixture decomposition; it came out in [[1]](#1). Note that in the following, we will use the words *concept* and *topic* interchangeably.\n",
    "\n",
    "Given word $w$ and document $d$, the probability of co-occurrence (as, the probability that $w$ occurs in $d$) is given by a mixture of conditionally independent multinomial distributions. Given latent categories (topics) $c$, we write indeed:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P(w, d) &= \\sum_c P(c) P(d | c) P(w | c) \\\\\n",
    "            &= P(d) \\sum_c P(c | d) P(w | c)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the fist formulation, $w$ and $d$ are thought as generated from the latent class $c$ in similar ways by using the conditional probabilities $P(d|c)$ and $P(w|c)$. In the second formulation instead, $\\forall d$, a latent class is chosen conditionally to it according to the probability $P(c|d)$ and a word is generated from that class following $P(w|c)$. The number of parameters is $c(w+d)$, so it grows linearly with the number of documents. Said parameters are estimated using the [EM algorithm](../../prob-stats/methods/em.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts, documents, words and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLSA is an improved variation of LSA in the sense that \n",
    "\n",
    "* documents might not contain similar terms but still refer to the same concept\n",
    "* queries can contain words not present in a document and still be very relevant to the document\n",
    "\n",
    "This is why is uses the probability of a word, or full query $q$ given the context ($r \\in \\{0, 1\\}$ is the relevance of the document):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P(r=1 | q) &= \\frac{P(q | r=1) P(r=1)}{P(q)} \\\\\n",
    "               &\\propto P(q|r=1) P(r=1)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where in the last writing we have $P(q|r=1)$, telling us given a document, how probable is the query, and $P(r=1)$, which can be uniform or dependent on the popularity of the document. \n",
    "\n",
    "$P(q | r=1)$ is calculated as:\n",
    "\n",
    "* $\\forall d$, compute the probability of each word $w$ to be relevant for it $P(w | r=1)$\n",
    "* compute the conditional probability of words in $q$\n",
    "\n",
    "The model relies on the idea that each concept has a distribution over words, each document is a mixture of concepts and each word is drawn from the topics. We will have, according to the equation above, \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P(w, d) &= P(d) \\sum_c P(w | c) P(c | d) \\\\\n",
    "            &= \\sum_c P(d|c) P(c) P(w|c)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and this is a factorisation of the original matrix into three matrices (hence the relation to LSA):\n",
    "\n",
    "* a matrix $U$ which maps documents to concepts\n",
    "* a matrix $\\Sigma$ which contains the concepts\n",
    "* a matrix $V$ which maps concepts to words\n",
    "\n",
    "But, differently from an SVD factorisation used in LSA, here there is no orthonormality condition for $U$ and $V$ and their elements are non-negative because they are probabilities. \n",
    "\n",
    "Now, we need to find all parameters such that the probability of observing the corpus is maximised. Using a [MLE](../../prob-stats/methods/mle.ipynb) approach, we need to maximise the likelihood\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\Pi_{i=1}^n \\Pi_{j=1}^m P(w_j, d_i)^{n(w_j, d_i)} \\ ,\n",
    "$$ \n",
    "\n",
    "where the exponent gives the multiplicity of $w_j$ in $d_i$, as in, their count. So, computing the log, \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\log \\mathcal{L} &= \\sum_{i=1}^n \\sum_{j=1}^m n(w_j, d_i) \\log P(w_j, d_i) \\\\\n",
    "                     &= \\sum_{i=1}^n \\sum_{j=1}^m n(w_j, d_i) \\log \\left[ \\sum_{k=1}^K P(d_i) P(c_k|d_i) P(w_j|c_k) \\right] \\\\\n",
    "                     &= \\sum_{i=1}^n n(d_i) \\sum_{j=1}^m \\frac{n(w_j, d_i)}{n(d_i)} \\log \\left[\\sum_{k=1}^K P(d_i) P(c_k|d_i) P(w_j|c_k) \\right] \\\\\n",
    "                     &= \\sum_{i=1}^n n(d_i) \\sum_{j=1}^m \\frac{n(w_j, d_i)}{n(d_i)} \\left( \\log P(d_i) + \\log \\left[ \\sum_k P(c_k| d_i) P(w_j|c_k) \\right]  \\right) \\\\\n",
    "                     &= \\sum_{i=1}^n n(d_i) \\left[ \\sum_{j=1}^m \\frac{n(w_j, d_i)}{n(d_i)} \\log P(d_i) + \\sum_{j=1}^m \\frac{n(w_j, d_i)}{n(d_i)} \\log \\sum_k P(c_k | d_i) P(w_j | c_k) \\right] \\\\\n",
    "                     &= \\sum_{i=1}^n n(d_i) \\left( \\log P(d_i) + \\sum_{j=1}^m \\frac{n(w_j, d_i)}{n(d_i)} \\log \\left[ \\sum_k P(c_k | d_i) P(w_j | c_k) \\right] \\right)\n",
    "\\end{align} \\ ,\n",
    "$$\n",
    "\n",
    "this because $n(w_j, d_i)$ is the count of $w_j$ in $d_i$ and $n(d_i)$ is the number of words in $d_i$, hence the semplification at the first addend, as $\\sum_j \\frac{n(w_j, d_i)}{n(d_i)} = \\sum_j P(w_j) = 1$.\n",
    "\n",
    "Because of the coupling elements given by the $c_k$, this is a hard optimisation problem and the solution can be found via the EM algorithm:\n",
    "\n",
    "* *E step*: calculate the posterior using the current estimates of the parameters:\n",
    "\n",
    "$$\n",
    "P(c_k|d_i, w_j) = \\frac{P(w_j, c_k | d_i)}{P(w_j | d_i)} = \\frac{P(w_j|c_k, d_i) P(z_k|d_i)}{\\sum_k P(w_j|c_i, d_i) P(c_i, d_i)}\n",
    "$$\n",
    "\n",
    "* *M step*: maximise the logarithm of the likelihood from the posteriors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a name=\"2\"></a> T Hofmann [**Probabilistic latent semantic indexing**](http://www.csie.ntu.edu.tw/~b97020/DSP/p50-hofmann.pdf) *Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, ACM* 1999\n",
    "2. <a name=\"wiki\"></a> [Wikipedia on PLSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
