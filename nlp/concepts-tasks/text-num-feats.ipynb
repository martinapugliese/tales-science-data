{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, config_ipython\n",
    "config_ipython()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Text as numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is unstructured, and in order for it to be used in a model, we typically need to find a numerical representation of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag of Words framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bag of Words framework (aka the \"BoW\") is possibly the simplest numerical representation of strings of text one could envisage.\n",
    "\n",
    "Wikipedia ([[1]](#1)) claims that an early reference to this name is present in Z Harris' paper *Distributional Structure*, of 1954 ([[2]](#2)). \n",
    "\n",
    "In BoW, a text is simply transformed into a \"bag\" (a multiset, that is, a set allowing for multiple occurrences) of the words composing it: this methods is very simplistic in that it disregards grammar and word order. \n",
    "\n",
    "You have a corpus of sentences. What you do is you take all the unique words in the corpus and for each of the sentences you count the occurrences of each of those words.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "Given a corpus composed by the two texts\n",
    "\n",
    "1. \"John likes watching movies. Mary likes movies too.\"\n",
    "2. \"John also likes watching football games.\"\n",
    "\n",
    "The list of unique words in it is [\"John\", \"likes\", \"watching\", \"movies\", \"also\", \"football\", \"games\", \"Mary\", \"too\"], there's 9 words.\n",
    "\n",
    "The two texts get encoded into the lists, 9-items long, of the occurrences counts of all of those words. Respecting the order we chose for the list of unique words, we have:\n",
    "\n",
    "1. [1, 2, 1, 2, 0, 0, 0, 1, 1]\n",
    "2. [1, 1, 1, 0, 1, 1, 1, 0, 0]\n",
    "\n",
    "This is because \"John\" (first item) appears once in the first text, \"likes\" appears twice in the first text, and so on.\n",
    "\n",
    "We can play around with this a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now ask you to give us three sentences, down here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me three sentences\n",
      "First sentence: John likes watching movies. Mary likes movies too.\n",
      "Second sentence: John also likes watching football games.\n",
      "Third sentence: John likes movies.\n"
     ]
    }
   ],
   "source": [
    "print('Give me three sentences')\n",
    "\n",
    "s1 = input(\"First sentence: \")\n",
    "s2 = input(\"Second sentence: \")\n",
    "s3 = input(\"Third sentence: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the list of unique words you provided in total ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words are:  ['John', 'too', 'watching', 'movies', 'games', 'Mary', 'football', 'likes', 'also']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate sentences, replace punctuation with space and split on space\n",
    "# Do the same for each single sentence (for later use)\n",
    "s = s1 + s2 + s3\n",
    "for sign in punctuation:\n",
    "    s = s.replace(sign, ' ')\n",
    "    s1 = s1.replace(sign, ' ')\n",
    "    s2 = s2.replace(sign, ' ')\n",
    "    s3 = s3.replace(sign, ' ')\n",
    "    \n",
    "# Create the unique words list\n",
    "unique_words = list(set(s.split()))\n",
    "\n",
    "print('unique words are: ', unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each sentence provided, we now compute its BoW representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence in BoW:  [1, 1, 1, 2, 0, 1, 0, 2, 0]\n",
      "First sentence in BoW:  [1, 0, 1, 0, 1, 0, 1, 1, 1]\n",
      "First sentence in BoW:  [1, 0, 0, 1, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "s1_bow, s2_bow, s3_bow = [], [], []\n",
    "\n",
    "for word in unique_words:\n",
    "    s1_bow.append(s1.count(word))\n",
    "    s2_bow.append(s2.count(word))\n",
    "    s3_bow.append(s3.count(word))\n",
    "\n",
    "print('First sentence in BoW: ', s1_bow)\n",
    "print('First sentence in BoW: ', s2_bow)\n",
    "print('First sentence in BoW: ', s3_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TF-IDF framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF stands for *term frequency - inverse document frequency* and it is an improvement over the BoW model.\n",
    "\n",
    "There are two main ingredients in the framework. Suppose we are considering a word (or *term*) $t$ and its presence in a set of documents (texts) $D = \\{d\\}$ (corpus). We have:\n",
    "\n",
    "* the *term frequency* $tf(t)$: a function of the number of times $t$ appears in a document $d$, quantifies the relevance of $t$ in $d$\n",
    "* the *inverse document frequency* $idf(d, D)$: an inverse function of the number of documents in the corpus in which $t$ appears, it quantifies how spread $t$ is across $D$, so that it will be used to scale down words which are very frequent across the texts\n",
    "\n",
    "The idea behind the $idf$ is that words which are very common in the whole corpus (say articles, for instance) carry little specific information and need to be weighted down.\n",
    "\n",
    "The TF-IDF is given as the product of these two components:\n",
    "\n",
    "$$\n",
    "tf-idf(t, d, D) = tf(t, d) \\cdot idf(t, D)\n",
    "$$\n",
    "\n",
    "The term frequency $tf$ can be given as either of:\n",
    "\n",
    "1. raw frequency (count of occurrences): $f_{t, d}$\n",
    "2. boolean frequency, $1$ if $t \\in d$, $0$ otherwise\n",
    "3. log-scaled frequency $1 + \\log(f_{t, d})$ or $0$ if $t \\not \\in d$\n",
    "4. augmented frequency: $\\frac{f_{t, d}}{\\max\\{f_{t',d'} : t'\\in D\\}}$ (the augmentation is intended as the division by the max of the raw frequencies in the document, so that we control for the bias towards very frequent words in a text)\n",
    "\n",
    "The inverse document frequency $idf$ (see [[3]](#3) for the original idea of scaling down words by their presence in the corpus) is given as\n",
    "\n",
    "$$\n",
    "idf(t, D) = \\log \\left[ \\frac{\\left|D\\right|}{\\left|\\{d \\in D: t \\in d\\}\\right|} \\right] \\ ,\n",
    "$$\n",
    "\n",
    "but typically a $1$ is added at the denominator to control for those cases where term does not exist in document so it is zero. So the $idf$ is the logarithm of ratio of the total number of document in corpus and the number of documents containing the term.\n",
    "\n",
    "The probabilistic interpretation of this choice ([[4]](#4)) is given by the fact that the probability that a given document $d$ contains term $t$ is written as\n",
    "\n",
    "$$\n",
    "P(t | d) = \\frac{\\left|\\{d \\in D: t \\in d\\}\\right|}{\\left|D\\right|} \\ ,\n",
    "$$\n",
    "\n",
    "so that the $idf$ contains the inverse of this probability. Furthermore, the choice of a logarithm is a natural one in Information Retrieval, where scoring function are sought to be additive, and also because of th [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) ([[5]](#5)).\n",
    "\n",
    "$$\n",
    "idf(t, D) = -\\log(P(t | d))\n",
    "$$\n",
    "\n",
    "The $idf$ is then the logarithm of the inverse relative document frequency. \n",
    "\n",
    "**An example**\n",
    "\n",
    "Let us consider a corpus $D$ with the two documents:\n",
    "\n",
    "* A: \"this is a sample\"\n",
    "* B. \"this is another example\"\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "idf(this, D) = \\log(2/2) = 0 \\ ,\n",
    "$$\n",
    "\n",
    "because the term \"this\" appears in two out of two documents, and \n",
    "\n",
    "$$\n",
    "idf(example) = \\log(2/1) \\ ,\n",
    "$$\n",
    "\n",
    "because \"example\" appears in one out of two documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a name=\"1\"></a> [Wikipedia on the Bag of Words model](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "2. <a name=\"2\"></a> Z S Harris, [Distributional Structure](http://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520), *Word* 10.2-3, 1954\n",
    "3. <a name=\"3\"></a> K Spärck Jones, [A statistical interpretation of term specificity and its application in retrieval](https://pdfs.semanticscholar.org/4f09/e6ec1b7d4390d23881852fd7240994abeb58.pdf), *J of Documentation* 28:1, 1972\n",
    "4. <a name=\"4\"></a> [Wikipedia on the TF-IDF model](https://en.wikipedia.org/wiki/Tf–idf)\n",
    "5. <a name=\"5\"></a> [A quora question on the choice of log in idf](https://www.quora.com/Why-do-we-take-the-logarithm-of-ratio-total-number-of-documents-number-of-documents-containing-the-term-while-calculating-inverse-document-frequency-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
