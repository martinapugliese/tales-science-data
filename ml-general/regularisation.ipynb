{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation\n",
    "\n",
    "Regularisation is the precedure whereby you add terms to the cost function your algorithm is trying to minimise in order to control for overfitting and in cases when the problem is ill-posed, like when there are multiple or no solutions to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge: $L_2$\n",
    "\n",
    "Ridge regression, or Tikhonov-Philips regularisation, adds a quadratic term to the cost function, hence the $L_2$ type.\n",
    "\n",
    "### In general terms\n",
    "\n",
    "Discussion borrowed from [wikipedia](https://en.wikipedia.org/wiki/Tikhonov_regularization). Suppose we want to solve the problem\n",
    "\n",
    "$$\n",
    "A x = b\n",
    "$$\n",
    "\n",
    "via [OLS](../supervised-learning/regression/linear-regression.ipynb#Estimators:-Ordinary-Least-Squares-(OLS). While OLS minimises the function $||Ax - b||^2$, Ridge regression minimises $||Ax - b||^2 + ||\\Gamma x||^2$, where $\\Gamma$ is called the *Tikhonov matrix*, usually chosen as a multiple of $\\mathbb{1}$, and serves the purpose of giving preference to solutions with small norms. The explicit solution is\n",
    "\n",
    "$$\n",
    "x = (A^t A + \\Gamma^t \\Gamma)^{-1} A^t b\n",
    "$$\n",
    "\n",
    "and the effect of the regularisation may be varied via the scale of the matrix $\\Gamma$. For $\\Gamma = 0$, we fall back on the OLS solution, provided $A^t A$ exists. \n",
    "\n",
    "### In linear regression\n",
    "\n",
    "In the context of linear regression, where $w$ are the parameters, the Ridge regularisation solves the problem\n",
    "\n",
    "$$\n",
    "\\min_w ||wx - y||^2 + \\beta ||w||^2 \\ \\ , \\ \\ \\beta \\geq 0\n",
    "$$\n",
    "\n",
    "where the regularisation term is a way to penalise large coefficients. This regularisation is usually applied when there are too many predictors (features), for instance when the number of input variables exceeds the number of observations; in such cases fitting the model without penalisation will result in large sizes for the coefficients so the added term controls for this behaviour.\n",
    "\n",
    "In fact, the Ridge problem is equivalent (thinking in terms of Lagrange multipliers) to minimising $||wx - y||^2$ under constraint\n",
    "\n",
    "$$\n",
    "||w||^2 < c \\ ,\n",
    "$$\n",
    "\n",
    "for some $c > 0$, which means constraining the sizes of the coefficients. Therefore, Ridge regression is equivalent to putting a threshold on the size of the parameters. The trade-off is that a large $w$ would give a better RSS but the penalty term would be higher. A small $w$ instead, would given a lower RSS and is preferable. \n",
    "\n",
    "The larger the $\\beta$ used, the more we want the coefficients close to 0. \n",
    "\n",
    "<img src=\"../imgs/ridge.jpg\" width=\"350\" align=\"left\" style=\"margin:0px 50px\"/>\n",
    "\n",
    "The figure shows the relation between an OLS and a Ridge solution, in the case of 2 dimensions. The ellipses are the contours of the residual sum of squares: the inner ellipse has the smallest RSS, which is minimised at the estimate point. The constraint in ridge regression corresponds to a circle. We are trying to minimise the ellipse size and the circle simultaneously so the estimate is the point of contact between the ellipse and the cicle. \n",
    "\n",
    "Ridge shrinks coefficients but does not nullify any. The solutions of a Ridge regression have to be found by solving ($E$ is the error function)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_j} = 2 \\sum_i (y_i - w_j X_{ij})(-X_{ij}) + 2 \\beta w_j = 0 \\ ,\n",
    "$$\n",
    "\n",
    "getting to\n",
    "\n",
    "$$\n",
    "w_j = \\frac{-\\sum_i X_{ij} y_i}{\\sum_i X_{ij}^2 + \\beta} = \\frac{w_j^0}{\\sum_i X_{ij}^2 + \\beta} \\ ,\n",
    "$$\n",
    "\n",
    "$w^0$ being the solution of a normal regression and the denominator being a function of $\\beta$. So it's apparent that Ridgse scales the coefficients by a constant factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO: $L_1$\n",
    "\n",
    "<img src=\"../imgs/reg-comparison.jpg\" width=\"350\" align=\"right\" style=\"margin:0px 50px\"/>\n",
    "\n",
    "LASSO *Least Absolute Shrinkage (and) Selection Operator* [[1]](#lasso) is a $L_1$ regularisation where the cost function to be minimised is\n",
    "\n",
    "$$\n",
    "||w x - y||^2 + \\alpha ||w||_1 \\ .\n",
    "$$\n",
    "\n",
    "This is equivalent (thinking in terms of Lagrange multipliers) to minimising $||w x - y||^2$\n",
    "\n",
    "subject to constraint\n",
    "\n",
    "$$\n",
    "||w|| < c \\ ,\n",
    "$$\n",
    "\n",
    "with $c$ being a tuning parameter.\n",
    "\n",
    "If $c$ is large enough, the regularsation has no effect and the solution is the OLS one; for a sufficiently small $c$ instead, the solutions are the shrunken versions of the OLS ones. LASSO shrinks some coefficients and sets some to 0\n",
    "\n",
    "The figure shows the different approaches of a $L_1$ and $L_2$ regularisation.\n",
    "\n",
    "LASSO solves \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_j} = 2 \\sum_i (y_i - w_j X_{ij})(-X_{ij}) + 2 \\beta = 0 \\ ,\n",
    "$$\n",
    "\n",
    "giving\n",
    "\n",
    "$$\n",
    "w_j = \\frac{-\\sum_i X_{ij} y_i - \\beta}{\\sum_i X_{ij}} = \\frac{w_j^0 - \\beta}{\\sum_i X_{ij}}\n",
    "$$\n",
    "\n",
    "So it's apparent that LASSO shifts the coefficients by a constant factor, allowing for the possibility to set some to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net\n",
    "\n",
    "Elastic net [[2]](#elasticnet) is a linear combination of $L_1$ and $L_2$ regularisations so that the function to be minimised is\n",
    "\n",
    "$$\n",
    "||wx - y||^2 + \\alpha_1 ||w_1|| + \\alpha_2 ||w_2||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a name=\"lasso\"></a> R Tibshirani, Robert, **Regression Shrinkage and Selection via the lasso**, *Journal of the Royal Statistical Society B*, 58:1, 1996\n",
    "2. <a name=\"elasticnet\"></a> H Zou, T Hastie, **Regularization and Variable Selection via the Elastic Net**, *Journal of the Royal Statistical Society B*, 67:2, 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
