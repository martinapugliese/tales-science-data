# Machine Learning: Model Assessment

Everything we can think of when we need to assess the quality of a Machine Learning model.

## Problems of a model

*What are the problems a model can suffer from?*

* [Overfitting](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/model-assessment/problems/overfitting.ipynb)

## Performance Metrics and techniques

*The metrics used to assess the quality of a model.*

* [Classification metrics](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/model-assessment/perf-metrics-techniques/classification-metrics.ipynb)
* [Cross Validation](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/model-assessment/perf-metrics-techniques/cross-validation.ipynb)


## Performance Diagnostics

*For a regression problem, how to go a little deeper and diagnose what could be wrong.*

* [What's in the residuals of a regression?](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/model-assessment/diagnostics/regr-residuals.ipynb)
* [The ROC curve](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/model-assessment/diagnostics/roc.ipynb)
