{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The possible outcomes of a binary classification\n",
    "\n",
    "If we are in a classification problem where the two classes are $1$ (call it positive) and $0$ (call it negative), our model can spit out either of them, but chances are some points will be classified wrongly. This leaves us with 4 possible situations in terms of how the points get classified: \n",
    "\n",
    "* $TP$: True Positives, those points which are predicted as $1$ and are actually $1$;\n",
    "* $TN$: True Negatives, those points which are predicted as $0$ and are actually $0$;\n",
    "* $FP$: False Positives, those points which are predicted as $1$ but are actually $0$;\n",
    "* $FN$: False Negatives, those points which are predicted as $0$ but are actually $1$\n",
    "\n",
    "<img src=\"../../imgs/metrics-class.pdf\" align=\"left\" width=\"400\" style=\"margin:20px 50px\"/>\n",
    "\n",
    "The figure depicts these groups visually. In the context of Information Retrieval, let us say that I run a query against my corpus of texts: some documents will be retrieved but not all of them will be a good match to my query. The positive class represents the match to the query. \n",
    "\n",
    "Now, the sum $TP + FN + FP + TN$ gives the number of all documents in the corpus. $TP + FP$ (the area of the ellipse) will give me the documents selected by the query; but the elements which are relevant to the query are instead given by the sum of the green areas, $TP + FN$. \n",
    "\n",
    "% TODO Add this (from scikit docs) Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples.\n",
    "% TODO see stuff in http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures, particularly for macro/micro\n",
    "\n",
    "% TODO link to the bias/variance tradeoff explanation and how is it related to underfitting/overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "It is sometimes called ``contingency table'' or ``error matrix''. It is meant to \\textit{visualize} performance. \n",
    "\n",
    "Again let us consider the simple case of two classes (binary classification). Then, the confusion matrix is represented in Table \\ref{table:confmat}.\n",
    "\n",
    "\\begin{table}[ht!]\\label{table:confmat}\n",
    "    \\centering\n",
    "\t\\begin{tabular}{ *4c *4c | *6c }\n",
    "    \\multicolumn{2}{c}{\\textbf{\\ \\ \\ \\ Predicted}} \\\\\n",
    "    \\midrule \\midrule\n",
    "    \\multirow{2}{*}{\\rotatebox{90}{\\textbf{Real}}}\n",
    "    & TP & FN \\\\\n",
    "    & FP & TN   \\\\\n",
    "    \\bottomrule\n",
    "\t\\end{tabular}\n",
    "\t\\caption{The Confusion Matrix: shortenings are meant to represent the number of data points falling in each group.}\n",
    "\\end{table}\n",
    "\n",
    "In the case of a multi-class classification problem with $n$ classes, the matrix will be a $n \\times n$ one where the diagonal contains the counts for items predicted in each class, and out of diagonal items will report the number of wrongly classified items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, Precision, Recall, Specificity\n",
    "\n",
    "The \\textit{accuracy} is defined as\n",
    "\n",
    "$$\n",
    "a = \\frac{TP + TN}{TP + TN +FP + FN} \\ ,\n",
    "$$\n",
    "\n",
    "and measures the number of correct predictions over the total of data points. The weak spot of the accuracy is that it attributes equal cost to both kind of errors, giving no insight on %TODO continue\n",
    "\n",
    "The \\textit{precision} is defined as\n",
    "\n",
    "$$\n",
    "p = \\frac{TP}{TP + FP} \\ ,\n",
    "$$\n",
    "\n",
    "and is the fraction of true positives over the total of points classified as positive. With reference to figure \\ref{fig:metrics-class-groups}, the precision gives the number of relevant items found by the model divided by the total number of items retrieved in the positive class. In the Information Retrieval context described above, it would measure ``how useful the results are''.\n",
    "\n",
    "$p=1$ says that all items labelled as belonging to class $1$ where actually in class $1$.\n",
    "\n",
    "The \\textit{recall} (also called \\textit{sensitivity}) is defined as\n",
    "\n",
    "$$\n",
    "r = \\frac{TP}{TP + FN} \\ ,\n",
    "$$\n",
    "\n",
    "and gives the fraction of true positives over the total of points belonging to the positive class. Again referring to figure \\ref{fig:metrics-class-groups}, the recall furnishes the number of relevant items found by the model divided by the total of existing documents in the positive class. In the Information Retrieval context described above, it would measure ``how complete the results are''.\n",
    "\n",
    "$r=1$ means that all items in real class $1$ where actually classed as in class $1$, but says nothing about items wrongly classified in class $1$.\n",
    "\n",
    "The relation between precision and recall is often inverse: it is possible to increase the one by reducing the other, so they have to be investigated together. The $F-score$ described later is a metrics taking both of them into account.\n",
    "\n",
    "\\\n",
    "\n",
    "\\textit{Example}\n",
    "\n",
    "Let us imagine there is a surgeon who needs to remove all cancerous cells from a patient to prevent regeneration. In the process, if healthy cells are removed as well, this would leave disgraceful lesions to the organs involved. \n",
    "The decision to increase recall at the cost of precision is one where more cells than needed are removed and ensure that all bad ones will go. The decision to increase precision at the cost of recall, on the other hand, would see the surgeon be more conservative and ensure only bad cells are removed, at the cost of not removing them all.\n",
    "\n",
    "\\\n",
    "\n",
    "The \\textit{specificity} is defined as \n",
    "\n",
    "$$\n",
    "s = \\frac{TN}{FP + TN} \\ ,\n",
    "$$\n",
    "\n",
    "and it is the symmetrical metric of precision but for the negative class.\n",
    "\n",
    "Sensitivity and specificity are also respectively called the \\textit{TPR} (True Positive Rate) and the \\textit{FPR} (False Positive Rate), which are the values used in a ROC analysis, see %TODO put ref to ROC section\n",
    "\n",
    "\\paragraph{Lift}\n",
    "\n",
    "The \\textit{lift} is defined as\n",
    "\n",
    "$$\n",
    "l = \\frac{\\frac{TP}{TP + FN}}{\\frac{TP + FP}{TP + FN + FP + TN}} \\ ,\n",
    "$$\n",
    "\n",
    "and is a ratio between the recall (ratio of correctly classified positive samples) and the ratio of positively classified samples in the whole dataset.\n",
    "\n",
    "The lift measures the strength of the classifier on the basis of the positive ($1$) samples which are correctly predicted. It is a metrics typically used in marketing % TODO continue from page 2/4, the example and so on\n",
    "\n",
    "%TODO ROC here?\n",
    "\n",
    "\\paragraph{F-score}\n",
    "\n",
    "In general, a \\textit{F-score} is defined as \n",
    "\n",
    "$$\n",
    "F_\\beta = (1 + \\beta^2)\\frac{pr}{\\beta^2p + r} \\ \\ \\beta > 0, \\beta \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "The most commonly seen metric in this class is the \\textit{F1-score}, which is just the harmonic mean of precision and recall:\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\frac{pr}{p + r}\n",
    "$$\n",
    "\n",
    "The F-score furnishes a way to weigh precision and recall differently: while the F1-score weighs them equally, the F2-score\n",
    "gives more weight to the precision and the F0.5-score does the reverse, for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class Accuracy\n",
    "\n",
    "The accuracy specified above is an example of a microaverage: all classes are treated on the same ground and this means that the information about the classification error in each is hidden. \n",
    "\n",
    "The per-class accuracy, on the other hand, is a macroaverage metric, and furnishes the average of the ratios of correctly classified samples per each of the classes. \n",
    "\n",
    "This metric is particularly useful when classes are imbalanced as the accuracy alone may give a quite distorded picture as the class with the highest number of samples will dominate the statistics. It is typically a good idea to look at both things anyway.\\newline\n",
    "\n",
    "\\textit{Example}\n",
    "\n",
    "Let us assume that we have a binary classifier, tested on $100$ samples for class $1$ and $200$ samples for class $0$ and whose performance reports these results:\n",
    "\n",
    "\\begin{itemize}\n",
    "\t\\item $TP = 80$;\n",
    "\t\\item $FN = 20$;\n",
    "\t\\item $FP = 5$;\n",
    "\t\\item $TN = 195$\n",
    "\\end{itemize}\n",
    "\n",
    "The accuracy gives $a = \\frac{80 + 195}{80 + 20 + 5 + 195} = 0.91$, so quite high. But this masks the fact that actually class $1$ has a misclassification rate of $20/100$ (class $0$ has a misclassification rate of $5/200$, so quite small). In terms of accuracies per class (the complements of these missclassification rates), class $1$ has $a_1 = 0.8$ and class $0$ has $a_0 = 0.975$, so that the per-class accuracy is \n",
    "\n",
    "$$\n",
    "a_p = \\frac{1}{2} \\left(0.8 + 0.975 \\right) = 0.88\n",
    "$$\n",
    "\n",
    "\\paragraph{Log-loss}\n",
    "\n",
    "This metric can be used when the probability of the classification of each sample is accessible and is defined as\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log p_i + (1 - y_i) \\log (1-p_i) \\right] \\ ,\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples, $y_i \\in {0, 1}$ is the class of sample with index $i$ and $p_i$ its classification probability.\n",
    "\n",
    "The log-loss is the cross-entropy between the distributions of true labels and the predictions. %TODO cross-entropy reference \n",
    "and is a measure of the unpredictability, factoring the noise coming from using a predictor rather than the true labels. The log-loss is then meant to be small. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
