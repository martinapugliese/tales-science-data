{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree builds (like, literally) a tree of decisions where at each split a decision is taken and is an algorithm which can be used both in regression and in classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept Learning System (CLS)\n",
    "\n",
    "This algorithm is the core of decision tree learning and the original paper outlining it dates back from the sixties [[1]](#cls).\n",
    "\n",
    "Let us briefly outline its procedure. Given a training set $C$ and classes $1/0$ (so we refer to a binary classification problem), the algorithm follows this routine:\n",
    "\n",
    "1. \n",
    "    * If $c > 0 \\ \\forall c \\in C$, build a $1$ node and halt;\n",
    "    * If $c < 0 \\ \\forall c \\in C$, build a $0$ node and halt;\n",
    "    * Otherwise, choose feature $f$ of data and create a decision node    \n",
    "2. Partition $C$ into $C_1, C_2, \\ldots, C_n$, a group for each value of feature $f$\n",
    "\n",
    "3. Apply 1. recursively $\\forall C_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Dichotomiser 3 (ID3)\n",
    "\n",
    "This one, whose original paper by Quinlan can be reached at [[2]](#id3), adds a feature selection heuristics to CLS, where the idea is selecting the feature which best separates the data. \n",
    "\n",
    "The best thing is, as in many cases, to use an example to illustrate how it works. The example we'll use is the same authentic one Quinlan uses in his paper, and can also be found in many places online, like [[here]](#weather).\n",
    "\n",
    "#### An example\n",
    "\n",
    "Suppose we want to build a tree to determine if tomorrow the weather is going to be good to play football. We are in possess of data for the last 2 weeks where the weather has been described by 4 attributes and where we recorder if we went to play or not:\n",
    "\n",
    "* the *outlook*: was it sunny/overcast/rainy?\n",
    "* the *temperature*: was is hot/mild/cold?\n",
    "* the *humidity*: was it high/normal?\n",
    "* the *wind*: was it weak/strong?\n",
    "\n",
    "The data we collected is in the following table, which we will refer to as $S$. \n",
    "\n",
    "| Day      | Outlook   | Temperature  | Humidity   | Wind     | Play ? |\n",
    "| -------  |:------   :| :-----------:| :---------:| :-------:| :-----:|\n",
    "| $d_1$    | sunny     | hot          | high       | weak     | NO     |\n",
    "| $d_2$    | sunny     | hot          | high       | strong   | YES    |\n",
    "| $d_3$    | overcast  | hot          | high       | weak     | YES    |\n",
    "| $d_4$    | rainy     | mild         | high       | weak     | YES    |\n",
    "| $d_5$    | rainy     | cold         | normal     | weak     | YES    |\n",
    "| $d_6$    | rainy     | cold         | normal     | strong   | NO     |\n",
    "| $d_7$    | overcast  | cold         | normal     | strong   | YES    |\n",
    "| $d_8$    | sunny     | mild         | high       | weak     | NO     |\n",
    "| $d_9$    | sunny     | cold         | normal     | weak     | YES    |\n",
    "| $d_{10}$ | rainy     | mild         | normal     | weak     | YES    |\n",
    "| $d_{11}$ | sunny     | mild         | normal     | strong   | YES    |\n",
    "| $d_{12}$ | overcast  | mild         | high       | strong   | NO    |\n",
    "| $d_{13}$ | overcast  | hot          | normal     | weak     | YES    |\n",
    "| $d_{14}$ | rainy     | mild         | high       | strong   | NO     |\n",
    "\n",
    "To measure how data is spread out, we calculate the [entropy](../../maths/cross-concepts.ipynb#Entropy)\n",
    "\n",
    "$$\n",
    "H(S) = -\\sum p \\log p \\ ,\n",
    "$$\n",
    "\n",
    "which, because $p = \\frac{9}{14}$ as there are $9$ out of $14$ YES, becomes\n",
    "\n",
    "$$\n",
    "H(S) = -\\frac{9}{14} \\log(\\frac{9}{14}) - \\frac{5}{14} \\log(\\frac{5}{14}) = 0.94 \n",
    "$$\n",
    "\n",
    "Now, given attribute $A$, the *Information Gain* $G(S, A)$ of S on it is defined as the difference in entropy before and after splitting the data on $A$:\n",
    "\n",
    "$$\n",
    "G(S, A) = H(S) - \\sum_{t \\in A} p(t) H(t) \\ ,\n",
    "$$\n",
    "\n",
    "where the sum goes over all the values of attribute $A$, $p(t)$ represents the fraction of the data with value $t$ for attribute $A$ and $H(t)$ is the entropy of the subset of data with value $t$ for attribute $A$. \n",
    "\n",
    "As an example, if we want to compute the gain in splitting the data on attribute **Wind**, which has two possible values, we consider that we have $p_{strong} = 6/14$ and $p_{weak} = 8/14$, then \n",
    "\n",
    "$$\n",
    "H(\\text{\"strong\"}) = -\\frac{3}{6} \\log(\\frac{3}{6}) - \\frac{3}{6}\\log(\\frac{3}{6}) \\ ,\n",
    "$$\n",
    "\n",
    "as there are 3 YES in subset of data with **Wind** attribute equal to *strong*. In the same way, \n",
    "\n",
    "$$\n",
    "H(\\text{\"weak\"}) = -\\frac{6}{8} \\log(\\frac{6}{8}) - \\frac{2}{8}\\log(\\frac{2}{8})\n",
    "$$\n",
    "\n",
    "This way we are able to compute the Information Gain related to each attribute:\n",
    "\n",
    "$$\n",
    "G(S, Outlook) = 0.246 \\ ; G(S, T) = 0.029 \\ ; G(S, Humidity) = 0.151 \\ ; G(S, Wind) = 0.048\n",
    "$$\n",
    "\n",
    "<img src=\"../../imgs/dectree.jpg\" align=\"left\" width=\"400\" style=\"margin:20px 50px\"/>\n",
    "\n",
    "We see that the **Outlook** is the attribute with the highest gain and this means that it gets chosen at the first node. It has branches for *sunny*, *overcast* and *rainy* which furnish the next level in the tree. So the next question is \"what attribute should be checked at the *sunny* branch node?\"\n",
    "\n",
    "At this point the dataset to be considered is that including *sunny*, which is $S_{\"sunny\"} = {D_1, D_2, D_8, D_9, D_{11}}$ and so we calculate the information gains\n",
    "\n",
    "$$\n",
    "G(S_{sunny}, T) = 0.570 \\ ; G(S_{sunny}, Humidity) = 0.970 \\ ; G(S_{sunny}, Wind) = 0.019 \n",
    "$$\n",
    "\n",
    "so the humidity gives the highest gain and furnishes the next decision node.\n",
    "\n",
    "This process goes on until all data is classified as we run out of attributes. Note that Other metrics than the Information Gain can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### C4.5\n",
    "\n",
    "It is a successor of ID3, removes restrictions over features having to be categorical. When it supports numerical target variables, we talk of CART trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C 5.0\n",
    "\n",
    "It is Quinlan's latest version, more accurate and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons of Decision Trees\n",
    "\n",
    "Cons are \n",
    "\n",
    "* Prone to overfitting (problem can be mitigated via pruning and setting the max depth or via training multiple trees in an ensemble as in a [Random Forest](rf.ipynb)\n",
    "* unstable: it is very dependent on the exact series of training examples\n",
    "* the optimal decision tree strategy is a NP problem, a practical strategy like ID3 is greedy and does optimal decisions at each node\n",
    "* If some classes dominate, the tree will be biased; it is in fact recommended to balance the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the impurity measures\n",
    "\n",
    "In the explanation above, the impurity measure used to split the data is the *entropy*, used to calculate the information gain. A competitor metric would be the *Gini impurity*, defined as\n",
    "\n",
    "$$\n",
    "I = \\sum p(1-p) \\ ,\n",
    "$$\n",
    "\n",
    "where the first factor ($p$) expresses the probability of an item being chosen and the second the probability of a mistake in the classification of the item. $I$ is maximal in the same point where the entropy $H = -\\sum p \\log p$ is maximal. The Gini impurity, named after the statistician and demographer C Gini, tells how often an element randomly chosen from the set would be incorrectly labelled if it were randomly labelled according to the distribution of labels in the set, hence furnishing a criterion to minimise the probability of a misclassification.\n",
    "\n",
    "The Gini impurity can be used in the information gain in place of the entropy. See [[4]](#4) for a discussion on the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a name=\"cls\"></a> E B Hunt, J Marin, P J. Stone, **Experiments in induction**, 1966\n",
    "2. <a name=\"id3\"></a> J R Quinlan, [**Induction of decision trees**](http://hunch.net/~coms-4771/quinlan.pdf), *Machine learning* 1.1, 1986\n",
    "3. <a name=\"weather\"></a> [Some slides illustrating the weather example](http://www4.stat.ncsu.edu/~dickey/Analytics/Datamine/Reference%20Papers/machine%20learning.pdf)\n",
    "4. <a name=\"raschka\"></a> [S Raschka on the entropy vs Gini impurity as the impurity metric](https://sebastianraschka.com/faq/docs/decision-tree-binary.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
