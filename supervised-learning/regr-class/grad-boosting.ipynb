{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "import cv2\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is and the idea\n",
    "\n",
    "The gradient boosting method, sometimes called the Gradient Boosting Machine (GBM), like other boosting methods, combines many weak learners into one strong learner in a iterative fashion. The weak learned it typically chosen as a decision tree, where the name *Gradient Tree Boosting* comes from in that case. The method works by optimising a differentiable loss function by means of grandient descent.\n",
    "\n",
    "The goal is learn a model $F$ that predicts \n",
    "\n",
    "$$\n",
    "\\hat y = F(x)\n",
    "$$\n",
    "\n",
    "by minimising a loss function $L$ (can be for instance the mean squared error averaged over the training set):\n",
    "\n",
    "$$\n",
    "L = ||\\hat y - y||^2\n",
    "$$\n",
    "\n",
    "where $y$ are the true values.\n",
    "\n",
    "Let's say we have a total of $M$ stages. At each stage $1 \\leq m \\leq M$, we use a weak learner $F_m$: an example of a (very) weak learner could be one that predicts the mean of all the sample points as the target.\n",
    "\n",
    "At the following stage, the GBM improves the learner by adding an estimator $h$ so that\n",
    "\n",
    "$$\n",
    "F_{m+1}(x) = F_m(x) + h(x)\n",
    "$$\n",
    "\n",
    "The point is how to choose the estimator $h$.\n",
    "\n",
    "A perfect $h$ would give \n",
    "\n",
    "$$\n",
    "F_{m+1}(h) = F_m(x) + h(x) = y \\Leftrightarrow h(x) = y - F_m(x)\n",
    "$$\n",
    "\n",
    "so $h$ represents the residual. As a matter of fact, the GBM fits $h$ to the residual. \n",
    "\n",
    "This way, like in other boosting methods, each learner learns to correct its predecessor. \n",
    "\n",
    "## The Mathematics\n",
    "\n",
    "In general, in a function space, given a training set of $N$ points\n",
    "\n",
    "$$\n",
    "\\{(\\mathbf x_1, y_1), (\\mathbf x_2, y_2), \\ldots, (\\mathbf x_N, y_N)\\} \\ ,\n",
    "$$ \n",
    "\n",
    "with $\\mathbf x_i = (x_i^1, x_i^2, \\ldots, x_i^n)$ a vector of $n$ features.\n",
    "\n",
    "Goal: finding a $\\hat F(x)$ as the approximation of a $F^*(x)$ that minimises the expected value of some chosen loss function $L(y, F(\\mathbf x))$ over the distribution of the training set values, so that\n",
    "\n",
    "$$\n",
    "F^*(x) = arg \\min_F \\mathbb{E}_{\\mathbf x, y} [L(y, F(\\mathbf x))]\n",
    "$$\n",
    "\n",
    "The problem will have to be solved by numerical optimization. We have the functional \n",
    "\n",
    "$$\n",
    "\\Phi(F) = \\mathbb{E}_{\\mathbf x, y} [L(y, F(\\mathbf x))] \\ ,\n",
    "$$\n",
    "\n",
    "which we can write as\n",
    "\n",
    "$$\n",
    "\\Phi(F) = \\mathbb{E}_{\\mathbf x} [\\phi(F(\\mathbf x))]\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\phi(F(\\mathbf x)) = \\mathbb{E}_y[L(y, F(\\mathbf x)) | \\mathbf x] \\ .\n",
    "$$\n",
    "\n",
    "This functional of $F$ has to be minimised over $F$, which is considered as a parameter. In numerical optimization, we will assume the form of the solution to be \n",
    "\n",
    "$$\n",
    "F(\\mathbf x) = \\sum_{m=0}^{M} f_m(\\mathbf x)\n",
    "$$\n",
    "\n",
    "$h_0$ is an initial guess and the subsquent $f_m$ are the incremental boosts we perform at each step.\n",
    "\n",
    "The optimization method utilised will be the gradient descent, whereby we compute the gradient at each step as\n",
    "\n",
    "$$\n",
    "\\mathbf g_m (\\mathbf x) = \\Big [ \\frac{\\partial \\phi(F(\\mathbf x))}{\\partial F(\\mathbf x)} \\Big ]_{F(\\mathbf x) = F_{m-1}(\\mathbf x)}\n",
    "$$\n",
    "\n",
    "and $F_{m-1}(\\mathbf x) = \\sum_{i=0}^{m-1} f_i(\\mathbf x)$.\n",
    "\n",
    "Assuming enough regularity that derivative and integration can be exchanged, we compute\n",
    "\n",
    "$$\n",
    "\\mathbf g_m(\\mathbf x) = \\mathbb{E}_y \\Big[\\frac{\\partial L(y, F(\\mathbf x))}{\\partial F(\\mathbf x)} \\Big| x \\Big]_{F(\\mathbf x) = F_{m-1}(x)}\n",
    "$$ \n",
    "\n",
    "By line-search we will have the parameter update\n",
    "\n",
    "$$\n",
    "F_m(\\mathbf x) = F_{m-1}(\\mathbf x) - \\rho_m \\mathbf g_m(\\mathbf x) \n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\rho_m = arg \\min_\\rho \\mathbb{E}_{y, \\mathbf x} [L(y, F_{m-1}(\\mathbf x)) - \\rho \\mathbf g_m(\\mathbf x)]\n",
    "$$\n",
    "\n",
    "In practice though, we have finite data, our training set. This means that $\\phi(F(\\mathbf x))$ cannot be estimated accurately by its value at each $\\mathbf x_i$, because we'd only be using the training set to estimate it. We'd need to use other points.\n",
    "\n",
    "This problem gets typically addressed by using a parametric form for $F$:\n",
    "\n",
    "$$\n",
    "F(\\mathbf x, \\{\\beta_m, \\mathbf a_m\\}_1^M) = \\sum_{m=1}^M \\beta_m h_m(\\mathbf x; \\mathbf a_m)\n",
    "$$\n",
    "\n",
    "and performing parameter optimization to minimise the training data-based approximation of the loss function, so that\n",
    "\n",
    "$$\n",
    "\\{\\beta_m, \\mathbf a_m\\}_1^M = arg \\min_{ (\\beta'_m, \\mathbf a'_m \\}_1^M } \\sum_{i=1}^N L\\Big(y_i, \\sum_{m=1}^M \\beta'_m h(\\mathbf x_i; \\mathbf a'_m) \\Big)\n",
    "$$\n",
    "\n",
    "When this is unfeasible, a greedy approach is utilised, so that, for a given $m$, \n",
    "\n",
    "$$\n",
    "(\\beta_m, \\mathbf a_m) = arg \\min_{\\beta, \\mathbf a} \\sum_{i=1}^N L\\Big(y_i, F_{m-1}(\\mathbf x_i) + \\beta h(\\mathbf x_i, \\mathbf a) \\Big)\n",
    "$$\n",
    "\n",
    "and then\n",
    "\n",
    "$$\n",
    "F_m(\\mathbf x) = F_{m-1}(\\mathbf x) + \\beta_m h(\\mathbf x; \\mathbf a_m)\n",
    "$$\n",
    "\n",
    "$h(\\mathbf x, \\mathbf a_m)$ is the weak learned employed.\n",
    "\n",
    "Also, we use the data-based computation of the gradient\n",
    "\n",
    "$$\n",
    "g_m(\\mathbf x_i) = \\Big[\\frac{\\partial L(y_i, F(\\mathbf x_i))}{\\partial F(\\mathbf x_i)} \\Big]_{F(\\mathbf x) = F_{m-1}(\\mathbf x)}\n",
    "$$\n",
    "\n",
    "and the data-based line-search update\n",
    "\n",
    "$$\n",
    "\\rho_m = arg \\min_\\rho \\sum_{i=1}^N L(y_i, F_{m-1}(\\mathbf x_i) + \\rho h(\\mathbf x_i; \\mathbf a_m))\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_m(\\mathbf x) = F_{m-1} (x) + \\rho_m h(\\mathbf x; \\mathbf a_m)\n",
    "$$\n",
    "\n",
    "## The Algorithmic Implementation\n",
    "\n",
    "1. Start initialising a constant function $F_0(x)$:\n",
    "$$\n",
    "F_0(\\mathbf x) = arg \\min_\\rho \\sum_{i=1}^{N} L(y_i, \\rho)\n",
    "$$\n",
    "\n",
    "\\item For $m=1$ to $M$:\n",
    "\t\\begin{itemize}\n",
    "\t\t\\item Compute the negative gradient for each data point $i=1, \\ldots, N$ as\n",
    "        $$\n",
    "\t\tg_{mi} = - \\Big[\\frac{\\partial L(y_i, F(\\mathbf x_i))}{\\partial F(\\mathbf x_i)} \\Big]_{F=F_{m-1}}\n",
    "\t\t$$\n",
    "        \\item Compute the parameters from the training set as per above\n",
    "        \\item Update the learner $F_m$ as above\n",
    "\t\\end{itemize}\n",
    "\n",
    "\\paragraph{Choosing the loss function}\n",
    "\n",
    "\\textit{Ordinary Least Squares Regression}\n",
    "\n",
    "In the simple case of an OLS regression, the loss function is\n",
    "\n",
    "$$\n",
    "L(y, F) = \\frac{1}{2}(y-F)^2 \\ ,\n",
    "$$\n",
    "\n",
    "so its negative gradient with respect to $F$ is $(y - F)$, that is, the residual. \n",
    "\n",
    "The update rule computes\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F_m &= F_{m-1} + h \\\\\n",
    "    &= F_{m-1} + y - F_m \\\\\n",
    "    &= F_{m-1} - \\frac{\\partial L}{\\partial F_m}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "so it clearly appears that the update rule is that of the Gradient Descent.\n",
    "\n",
    "\\paragraph{Gradient Tree Boosting}\n",
    "\n",
    "When used with a decision tree as the weak learner (usual case), the algorithm is called Gradient Tree Boosting. \n",
    "\n",
    "Note that with respect with normal GB, GTB does not use the same $\\rho_m$ for the whole tree but rather a $\\rho_{mj}$ for each leaf $j$.\n",
    "\n",
    "% TODO Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a name=\"paper\"></a> J H Friedman, [**Greedy function approximation: a gradient boosting machine**](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf), *Annals of Statistics*, 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
