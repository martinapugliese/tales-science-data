# Word Embeddings

_Word embeddings! Words and documents become vectors, that you can embed in a mathematical space and compute similarities, add and subtract together!_

Word embeddings are a set of models in NLP such that words \(or phrases, because they can be extended to them\) are mapped to numerical vectors. This way, they are rendered in numerical representations and can be used to derive similarities and predictions. These models are based on neural networks.

