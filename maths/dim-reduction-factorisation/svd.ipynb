{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, config_ipython\n",
    "config_ipython()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What is\n",
    "\n",
    "A singular-value decomposition (SVD) of a high-dimensional matrix gives a low-dimensional representation of it. Given a matrix $M$, an $m \\times n$ matrix of rank $r$, we can find matrices $U$, $\\Sigma$ and $V$ such that we have the decomposition $M = U \\Sigma V^t$. These matrices respect conditions:\n",
    "\n",
    "1. $U$ is an $m \\times r$ matrix and is column orthonormal (each column is a unit vector and the dot product of any two columns is $0$);\n",
    "2. $V$ is a $n \\times r$ matrix, also column orthonormal; \n",
    "3. $\\Sigma$ is a diagonal matrix and the elements on its diagonal are called the *singular values* of $M$.\n",
    "\n",
    "$U$, $\\Sigma$ and $V$ are called the SVD components of $M$. The singular values represent \"hidden concepts\" which connect the two other matrices together. SVD is often employes in recommender systems. \n",
    "\n",
    "### A small example, in words\n",
    "\n",
    "If $M$ contains the ratings given by people to movies, so that people are on the rows and movies on the columns, then $U$ connects people to concepts; $V$ connects movies to concepts and $\\Sigma$ gives the strength of each concept.\n",
    "\n",
    "A person not represented in $M$ wants to know what movies he/she would like based on the ones he/she has seen. If $q$ is the vector representing the movies the person rated, $q V$ maps the person to the concept space and this can be mapped back to the movie space by multiplying by $V^t$. \n",
    "\n",
    "To find similar users to a given user, we can map all users to the concept space using $V$ and then apply some similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where is the dimensionality Reduction part\n",
    "\n",
    "If $M$ is a very large matrix, its SVD components will be large as well. But in real-world uses of the technique, approximate decompositions are employed instead, so to reduce the original dimensionality. \n",
    "\n",
    "Setting the $s$ smallest singular values to $0$ and eliminating the corresponding $s$ rows of $U$ and $V$ we obtain an approximate representation. This procedure works well because it can be shown that it minimises the root mean square error between $M$ and its approximation, or the [Frobenius norm](../generic/matrices.ipynb) of this difference.\n",
    "\n",
    "Let $M = PQR$ be a generic decomposition, then $m_{ij} = \\sum_k \\sum_l p_{ik} q_{kl} r_{lj}$ and then \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "||M||^2 &= \\sum_i \\sum_j m_{ij}^2 \\\\\n",
    "&= \\sum_i \\sum_j (\\sum_k \\sum_l p_{ik} q_{kl} r_{lj})^2 \\\\\n",
    "&= \\sum_i \\sum_j \\sum_k \\sum_l \\sum_n \\sum_m p_{ik} q{kl} r_{lj} p_{in} q_{nm} r_{mj}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Now, if $P$, $Q$ and $R$ are the SVD components of $M$, then it means $Q$ is diagonal, $P$ is column orthonormal, $R$ is row orthonormal (note the transpose in the SVD equation), so $||M||^2 = \\sum_k q_{kk}^2$.\n",
    "\n",
    "Calling $P = U$, $Q=\\Sigma$ and $R=V^t$, with $\\sigma_i$ being the $i$-th diagonal element of $\\Sigma$, if we keep the first $n$ elements of $\\Sigma$ and set the remaining ones to $0$, obtaining  a $\\Sigma'$, we get\n",
    "\n",
    "$$\n",
    "M' = U \\Sigma' V^t \\ ,\n",
    "$$\n",
    "\n",
    "which is an approximation of $M$. The matrix giving the resulting errors is $M - M' = U (\\Sigma - \\Sigma') V^t$, whose norm is (following from the above)\n",
    "\n",
    "$$\n",
    "||M - M'||^2 = \\sum_k (\\Sigma - \\Sigma')_{kk}^2\n",
    "$$\n",
    "\n",
    "The matrix $\\Sigma - \\Sigma'$ has $0$ in the first $n$ diagonal elements and $\\sigma_i$ in the remaining ones, with $n < i \\leq r$. So, $||M - M'||^2$ is the sum of the squares of the elements of $\\Sigma$ which were set to $0$. In order to minimise it, we pick the smallest elements of $\\Sigma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the SVD of a matrix\n",
    "\n",
    "The computation is strictly connected to the eigenvalues of the symmetric matrix $M^tM$ and $M M^t$.\n",
    "\n",
    "If $M = U \\Sigma V^t$, then $M^t = V \\Sigma^t U^t$ and because $\\Sigma$ is diagonal, then $\\Sigma = \\Sigma^t$, so $M^t = V \\Sigma U^t$, which then means (using the orthonormality of $U$ and $V$)\n",
    "\n",
    "$$\n",
    "M^t M = V \\Sigma U^t U \\Sigma V^t = V \\Sigma^2 V^t\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "M^t M V = V \\Sigma^2\n",
    "$$\n",
    "\n",
    "Now, $\\Sigma^2$ is diagonal, with entries being the squares of $\\Sigma$'s entries. This last equation says that $V$ is the matrix of eigenvectors of $M^tM$ and $\\Sigma^2$ is the matrix of eigenvalues. So, by computing $M^tM$ we have the eigenvectors and the singular values, only $U$ remains.\n",
    "\n",
    "Now,\n",
    "\n",
    "$$\n",
    "M M^t = U \\Sigma V^t V \\Sigma U^t = U \\Sigma^2 U^t\n",
    "$$\n",
    "\n",
    "so $M M^t U = U \\Sigma^2$\n",
    "\n",
    "which means that $U$ is the matrix of eigenvectors of $M M^t$. \n",
    "\n",
    "$M M^t$ is a $n \\times m$ matrix, $M M^t$ is $m \\times n$ and $n, m \\geq r$. Hence, $M^t M$ and $M M^t$ have additional $n-r$ and $m-r$ eigenvectors which do not show up in $U$, $V$  and $\\Sigma$. Since $r$ is the rank, all other eigenvaules are $0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
