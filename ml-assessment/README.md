# Machine Learning: Model Assessment

*Techniques and methodologies to measure the quality of a model and to fix and improve it.*


## Generic problems models can have

*What are the problems a model can suffer from?*

* [**Overfitting and Underfitting**](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/ml-assessment/overfitting-underfitting.ipynb)


## Performance metrics and validation techniques

*The metrics used to assess the quality of a model and the techniques to do so.*

* [**Evaluating a model: concepts and techniques**](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/ml-assessment/perf-metrics-techniques/evaluating-model-concepts-techniques.ipynb)
* [**Classification performance metrics**](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/ml-assessment/perf-metrics-techniques/classification-metrics.ipynb)
* [**Regression performance metrics**](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/ml-assessment/perf-metrics-techniques/regression-metrics.ipynb)


## Deeper diagnostics

*Let's go a little deeper than using metrics and diagnose what could be wrong in a broader way.*

* [**What's in the residuals of a regression?**](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/ml-assessment/diagnostics/regr-residuals.ipynb)
* [**The ROC curve**](http://nbviewer.jupyter.org/github/martinapugliese/tales-science-data/blob/master/ml-assessment/diagnostics/roc.ipynb)
* [**Bias and Variance: a compromise**]()
