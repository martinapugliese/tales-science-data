{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks in a nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gist of what they are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artificial neural network (often shortened as ANN) is an attempt at artificially representing the network of neurons in the brain and their functioning with software. It consists of neural units (*artificial neurons*), each capable of receiving input and producing output, and which are connected together. \n",
    "\n",
    "The fun thing about ANNs, and the whole point of them, is the fact that they mimic \"learning\". Inputs to the network are weighted and a learning mechanism consists in an iterative self-adjustment of these weights in such a way to achieve optimal correspondence to the desired result on training data. This way, the network is meant to emulate the synapses of the brain in their capability to carry information to one neuron to another. \n",
    "\n",
    "### A bit on the biological link\n",
    "\n",
    "The biological metaphor of these algorithms to actual neural networks in the brain is more of an inspiration than a grounded reality. ANNs were conceived with the idea to mimic how the human brain works but in reality they are a far shout from actually doing this well and comprehensively, and also, we don't know the human brain well enough yet anyway. In fact, it is actually confusing to say that neural networks \"mimick the brain\", as the brain doesn't really work as they do. On this, the discussion in the opening chapter of [Chollet's book](#chollet) is a very good one.\n",
    "\n",
    "Following this [page](#biology), we can say that, in general:\n",
    "\n",
    "* real neurons are slower than artificial ones, but there's really plenty and the way they communicate is non-trivial\n",
    "* real networks use energy very efficiently\n",
    "* real networks can do several highly complex operations at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neurons and networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The artificial neuron\n",
    "\n",
    "<img src=\"../../imgs/ann.jpg\" width=\"500\" align=\"left\" style=\"margin:20px\"/>\n",
    "\n",
    "This here in this figure is a schematic model of an artificial neuron. \n",
    "\n",
    "Several input data $\\mathbf{x} = (x_1, \\ldots, x_n)$, represented by the $i$, are streamed into the neuron and combined with *weights* $\\mathbf{w} = (w_1, w_2, \\ldots, w_n)$, so that they get weighted, in a linear combination $\\mathbf{w} \\cdot \\mathbf{x}$. The neuron is also equipped with a threshold $t$, so that the combination becomes $\\mathbf{w} \\cdot \\mathbf{x} + t$, or $\\mathbf{w} \\cdot \\mathbf{x} -b$ where $b = -t$ is the bias. Note that if we assume a further input poin $x_0=1$ and use the bias as a further weight, we can write the combination as $\\mathbf{w} \\cdot \\mathbf{x}$ where $\\mathbf{w} = (b, w_1, w_2, \\ldots, w_n)$ and $\\mathbf{x} = (x_0, x_1, x_2, \\ldots, x_n)$.\n",
    "\n",
    "This weighted combination of inputs is passed to the $activation function$, which determines what is the output of the neuron. The activation function is what determines how the neuron fires. It takes the weighted input and determined what is the output, there exist different implemented choices for the activation function of a neuron/network. The simplest choice would be a linear function, but it can be shown that it wouldn't be a smart choice as it does not really allow the neuron to learn. This point and more about activations will be explored in other sections.\n",
    "\n",
    "### A network of neurons\n",
    "\n",
    "<figure style=\"float:right;\">\n",
    "  <img src=\"../../imgs/nielsen_ann.png\" width=\"400\" align=\"center\" style=\"margin:30px 70px\"/>\n",
    "  <figcaption>A (feedforward, see later) neural network. Image from [Nielsen's book](#nielsen).</figcaption>\n",
    "</figure>\n",
    "\n",
    "To build a network of neurons, what you have to do is put several of them together in a way that they can communicate. Neurons are grouped into *layers*, groups that reside at the same level, so that communication is passed from one layer of neurons to the other. In a network, there is \n",
    "\n",
    "* an *input* layer: the one at the start of the communication process\n",
    "* an *output* layer: the one that spits the final result at the end of the process\n",
    "* one or more *hidden* layers: the layers in between that constitute the intermediate steps\n",
    "\n",
    "Each layer can be composed of however many neurons you wish. This means that if there are $n$ neurons at a given stage, each neuron in the following stage will receive $n$ inputs. In much the same way as the transfer function uses a combination of weighted inputs into a neuron, the input to any neuron in a certain layer is a weighted sum of all outputs of the neurons of the previous layer. \n",
    "\n",
    "The network will learn by minimising a *loss function* via optimisation (typically gradient descent but not necessarily): this is the job of the *optimiser*, which regulates how exactly learning is performed. In essence, it applies gradient descent, with specific variants. The network gets its training started with weights initialised with some choice, say a random one. Training means allowing it to learn the best combination of weights that adheres to the input data.\n",
    "\n",
    "The optimiser is also paired with a mechanism called *backpropagation*, which acts during gradient descent and allows the weights to be adjusted continuouly in order to iteratively improve the accuracy of the results (minimising the loss function). What backpropagation does in practice is computing the derivatives of the cost function with respect to the weights and propagating them from the last layer back to the first one. Each weight gets modified iteratively by an amount which is proportional to the derivative of the cost function with respect to it, which is what gradient descent uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Universality of neural networks\n",
    "\n",
    "It can be shown (well, it's been mathematically [proven](#universality)) that neural networks can be \"taught\" to approximate any continuous function: the more neurons, the better the approximation achieved. This result is called the *Universal Approximation Theorem*. A very good blog post on the topic is [this one](#universality-blog), and the references below contain some other good reads as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of ANNs and pills of history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This here is no more than a super-quick and very high-level introduction to several types of neural networks one can buiild, the details of which are explored elsewhere in this chapter. You can find a more comprehensive outline of the different types of networks in this [article](#nets), which contains a very cool illustration by F Van Veen. The article also reports some important papers about the mentioned networks.\n",
    "\n",
    "The categories listed here describe different properties of the network, so categories can be combined to build networks. For example, you can build a deep convolutional network. The things loosely described down here will be expanded in the respective places in the chapter.\n",
    "\n",
    "### Deepness\n",
    "\n",
    "A network is called *deep* simply when it contains more than one hidden layer, usually many for today's applications, allowing for learning incredibly complex patterns in the data. Deep neural networks are those beasts performing *deep learning*, this (relatively) new trend in machine learning/artificial intellingence which is starting to tackle very complicated problems with impressive results. \n",
    "\n",
    "Deep Learning as a thing (a field) is not a new concept, it dates its birth back from the 1980s, but their big resurgence has been from the id 2000s, on when they have been finally shown to be capable to learn in an efficient way. Before then research on deep architectures hadn't reached the point where these tools could be put to use for any practical reason, due to time complexity, hardware bottlenecks, and overall lack of efficiency. \n",
    "\n",
    "### Learning architecture\n",
    "\n",
    "#### Feedforward \n",
    "\n",
    "In a feedforward network, communication flows in a horizontal way, meaning the output of neurons in a certain layer is passed to neurons in the next layer horizontally, there is no going backwards. Feedforward networks were conceived straight with the birth of the perceptron (the first ANN born), so in the 1950s. \n",
    "\n",
    "#### Recurrent \n",
    "\n",
    "Recurrent networks have loops, so the output of a neuron can be fed back to the neuron itself, allowing for the dynamicity which is missing in the feed-forward model. These types of networks are implemented in such a way that there is the time factor embedded in, meaning neurons fire only within a specific window of time, allowing for feedback communication to not be propagated instantaneously (which would be difficult to control). These types of networks have a concept of *memory* and there's several types of them. \n",
    "\n",
    "Recurrent networks were born in the 1980s. They are particularly suited for problems which involve the temporal component, like those dealing with natural language. \n",
    " \n",
    "### Topology\n",
    "\n",
    "#### Convolutional networks\n",
    "\n",
    "Convolutional networks have one or more convolutional layers, where a neuron is not connected to all other ones and the output is obtained via a convolution operation on the input data. Unlike fully connected networks, here neurons in a layer are only connected to a group of neurons in the previous layer and this makes for a very nice functionality.\n",
    "\n",
    "Convolutional networks are well suited for tasks related to vision, that is, where the input data consists of images or video data: for these sorts of tasks, in most typical case, a fully connected network would have to perform too many operations and be too large to be of any practical use (a neuron there, for an image of size 32x32 and in RGB, so with a 3 dimensional colour space, would have $32\\cdot32\\cdot3$ weights), while the use of convolutions saves complexity. \n",
    "\n",
    "The inspiration for these categories of networks came from the vision systems of the biological world, and this is why they have been designed specifically for machine vision tasks. An image gets passed to the network in batches of input data: at the very start, the first batch of $n$ pixels gets in, then a counter is shifted by one pixel and the second batch of $n$ pixels goes in. This mechanism is loosely borrowed from what the neurons in the visual cortex do. They only deal with a certain part of the visual field at once, that is, with a pixel and its neighbours. \n",
    "\n",
    "The first convolutional networks date from the 1990s (even though the concepts are decades older) but they became ubiquitous in the 2010s with the many visual applications they serve nowadays. In fact, they are particularly suited for image tasks as exhibit a natural ability to capture spatial structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "\n",
    "1. <a name=\"chollet\"></a> F Chollet, **Deep Learning with Python**, *Manning*, 2017\n",
    "2. <a name=\"nielsen\"></a> M Nielsen, [**Neural networks and deep learning**](http://neuralnetworksanddeeplearning.com), Determination Press, 2015\n",
    "3. <a name=\"nets\"></a> [**The neural network zoo**](http://www.asimovinstitute.org/neural-network-zoo/), an article + illustration by F Van Veen at the Asimov Institute\n",
    "\n",
    "### About artificial neurons\n",
    "\n",
    "1. []()\n",
    "\n",
    "\n",
    "### About the biological link\n",
    "\n",
    "1. <a name=\"biology\"></a> [The biological inspiration for ANNs](http://read.pudn.com/downloads164/doc/747044/L01.pdf), lecture from a course in Machine Learning by A. Papli≈Ñski\n",
    "\n",
    "### About the universality theorem\n",
    "\n",
    "1. <a name=\"universality\"></a> G Cybenko, [**Approximation by superposition of sigmoidal function**](http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf), *Math Control Signal System*, 2, 1989\n",
    "2. <a name=\"universality-blog\"></a> D McNeela [**The Universal Approximation Theorem for Neural Networks**](http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html)\n",
    "3. <a></a> J Klein [**A Simple Proof of the Universal Approximation Theorem**](https://blog.goodaudience.com/neural-networks-part-1-a-simple-proof-of-the-universal-approximation-theorem-b7864964dbd3)\n",
    "4. B Fortuner, [**Can neural networks really learn any function?**](https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6)\n",
    "\n",
    "### About types of networks\n",
    "\n",
    "1. [**Convolutional Neural Networks for visual recognition**](http://cs231n.github.io/convolutional-networks/), a Stanford CS class\n",
    "\n",
    "### Something funky\n",
    "\n",
    "1. <a name=\"series\"></a> [**Comparison of artificial neural networks and human brains on solving number series**](http://www.cogsys.wiai.uni-bamberg.de/teaching/ws1112/km/practice/ANNs.pdf), a students' project at the University of Bamberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
