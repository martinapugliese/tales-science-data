{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks in a nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gist of what they are\n",
    "\n",
    "An artificial neural network (often shortened as ANN) is, well, an attempt at artificially representing the network of neurons in the brain and their functioning with software. \n",
    "It consists of neural units (*artificial neurons*), each capable of receiving input and producing output based on rules, and which are connected together. \n",
    "\n",
    "The fun thing about ANNs, and the whole point of them, is the fact that they mimic \"learning\". Inputs to the network are weighted and the learning mechanism consists in an iterative self-adjustment of said weights in such a way to achieve optimal correspondence to the desired result on training data. This way, the network is meant to emulate the synapses of the brain in their capability to carry information to one neuron to another. \n",
    "\n",
    "### A bit on the biological link\n",
    "\n",
    "The biological metaphor of these algorithms to actual neural networks in the brain is more of an inspiration than a grounded reality. ANNs were conceived with the idea to mimic how the human brain works but in reality they are a far shout from actually doing this comprehensively, and also, we don't know the human brain well enough yet anyway. \n",
    "\n",
    "Following [[1]](#biology), we can say that, in general:\n",
    "\n",
    "* real neurons are slower than artificial ones, but there's really plenty and the way they communicate is non-trivial\n",
    "* real networks use energy very efficiently\n",
    "* real networks can do several highly complex operations at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neurons and how they work\n",
    "\n",
    "<img src=\"../imgs/ann.jpg\" width=\"500\" align=\"left\" style=\"margin:20px\"/>\n",
    "\n",
    "This here in this figure is the generic and schematic model of an artificial neuron. Several input data $(x_1, \\ldots, x_n)$ are streamed into the neuron and a *transfer function* (which we indicate with $f$, note that it can also be called *activation function*) combines them with weights $(w_1, w_2, \\ldots, w_n)$ (usually in a linear combination) to determine what the neuron computes. Then an *output function* spits an output of the neuron based on a threshold $t$ the neuron is equipped with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A network of neurons\n",
    "\n",
    "<figure style=\"float:right;\">\n",
    "  <img src=\"../imgs/nielsen_ann.png\" width=\"400\" align=\"center\" style=\"margin:30px 70px\"/>\n",
    "  <figcaption>A (feedforward, see later) neural network. Image from [[2]](#book).</figcaption>\n",
    "</figure>\n",
    "\n",
    "To build a network of neurons, what you have to do is put several of them together in a way that they can communicate. Neurons are grouped into *layers*, groups that are at the same level, so that communication is passed from one layer of neurons to the other. In a network, there is \n",
    "\n",
    "* an *input* layer: the one at the start of the communication process\n",
    "* an *output* layer: the one that spits the final result at the end of the process\n",
    "* one or more *hidden* layers: the layers in between that constitute the intermediate steps\n",
    "\n",
    "Each layer can be composed of however many neurons you wish. This means that if there are $n$ neurons at a given stage, each neuron in the following stage will receive $n$ inputs.\n",
    "\n",
    "In much the same way as the transfer function uses a combination of weighted inputs into a neuron, the input to any neuron in a certain layer is a weighted sum of all outputs of the neurons of the previous layer. \n",
    "\n",
    "### Types of ANNs and pills of history\n",
    "\n",
    "This here is no more than a super-quick and very high-level intro to several types of neural networks, the details of which are explored elsewhere in this chapter. You can find a more comprehensive outline of the different types of networks in [[2]](#nets), even with a great and coloured illustration by F Van Veen. The article also reports some important papers about the mentioned networks.\n",
    "\n",
    "Note that the categories of networks listed here are not necessarily mutually exclusive, because they may describe different properties of the network. For example, feedforward networks can be deep or not deep.\n",
    "\n",
    "#### Feedforward networks \n",
    "\n",
    "In a feedforward network, communication flows in a horizontal way, meaning the output of neurons in a certain layer is passed to neurons in the next layer horizontally, there is no going backwards. \n",
    "The figure helps clarifying the situation: here you have 3 neurons in the input layer, one in the output layer and 4 in the (single) hidden layer.\n",
    "\n",
    "Feedforward networks of artificial neurons were conceived straight with the birth of the [perceptron](types/perceptron.ipynb), so in the 1950s. Also, this categorisation here is not comprehensive at all. \n",
    "\n",
    "#### Recurrent networks\n",
    "\n",
    "Recurrent networks have loops, so the output of a neuron can be fed back to the neuron itself. These types of networks are implemented in such a way that there is the time factor embedded in, meaning neurons fire only within a specific window of time, allowing for feedback communication to not be propagated instantaneously (which would be difficult to control). They work well with dynamic tasks where there is a time component.\n",
    "\n",
    "Recurrent networks were born in the 1980s.\n",
    "\n",
    "#### Deep (and shallow) networks\n",
    "\n",
    "Deep neural networks are those beasts performing *deep learning*, this (relatively) new trend in machine learning/artificial intellingence which is starting to tackle very complicated problems with impressive results. Being deep for a network means nothing more than having several hidden layers, allowing for enormous complexity. Networks that are not deep are called *shallow*.\n",
    "\n",
    "Deep Learning as a thing (a field) is not a new concept, it dates its birth back from the 1980s, but their big resurgence has been in the year 2006 when they have been finally shown to be capable to learn in an efficient way. Before then research on deep artichitectures hadn't reached the point where these tools could be put to use for any practical reason, due to time complexity and overall lack of efficiency.  \n",
    "\n",
    "#### Convolutional networks\n",
    "\n",
    "Convolutional networks are deep and feedforward. In the convolutional layers of these networks not every neuron is connected to every other neuron and a the output is obtained via a convolution operation on the input data. Convolutional networks are well suited for tasks related to vision, that is, where the input data consists of images: for these sorts of tasks, in most typical case, a \"normal\" feedforward networks would have to perform too many operations and be too large to be of any practical use, while the use of convolutions saves complexity. \n",
    "\n",
    "The inspiration for these categories of networks came from the vision systems of the biological world, and this is why they have been designed specifically for machine vision tasks. An image gets passed to the network in batches of input data: at the very start, the first batch of $n$ pixels gets in, then a counter is shifted by one pixel and the second batch of $n$ pixels goes in. This mechanism is loosely borrowed from what the neurons in the visual cortex do. They only deal with a certain part of the visual field at once, that is, with a pixel and its neighbours. \n",
    "\n",
    "The first convolutional networks date from the 1990s but they became ubiquitous in the 2010s with the many visual applications they serve nowadays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a network\n",
    "\n",
    "The training process of a network is not different from the general training process of supervised algorithms: you want to minimise a cost function that gives a comprehensive measurement of the mistakes (differences between predicted points and real points). You do this via [gradient descent](../maths/generic/gradient-descent.ipynb), which minimises the error function, that is to say the difference between the expected value of the output and the value the network outputs.\n",
    "\n",
    "Neural network are equipped with a mechanism called [backpropagation](backpropagation.ipynb), which acts during gradient descent and allows the weights to be adjusted continuouly in order to iteratively improve the accuracy of the results. What backpropagation does in practice is computing the derivatives of the cost function with respect to the weights and propagating them from the last layer back to the first one. Each weight gets modified iteratively by an amount which is proportional to the derivative of the cost function with respect to it, which is what gradient descend uses.\n",
    "\n",
    "Weights, at the very starting stage of training, get initiliased at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a name=\"biology\"></a> [The biological inspiration for ANNs](http://read.pudn.com/downloads164/doc/747044/L01.pdf), lecture from a course in Machine Learning by A. Papliński\n",
    "2. <a name=\"book\"></a> M Nielsen, [**Neural networks and deep learning**](http://neuralnetworksanddeeplearning.com), Determination Press, 2015\n",
    "3. <a name=\"nets\"></a> [**The neural network zoo**](http://www.asimovinstitute.org/neural-network-zoo/), an article + illustration by F Van Veen at the Asimov Institute\n",
    "4. <a name=\"series\"></a> [Comparison of artificial neural networks and human brains on solving number series](http://www.cogsys.wiai.uni-bamberg.de/teaching/ws1112/km/practice/ANNs.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
