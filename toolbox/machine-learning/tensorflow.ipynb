{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common_methods/import_all.py\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy.integrate import quad, odeint\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import detrend\n",
    "from scipy.spatial import distance\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import tensorflow as tf\n",
    "\n",
    "from common_methods.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TensorFlow playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main concepts\n",
    "\n",
    "Calling the library `tf`, from a `import tensorflow as tf`.\n",
    "\n",
    "### Components\n",
    "\n",
    "* `tf.Graph`: the computational program, made of `tf.Tensor` and `tf.Operation` objects\n",
    "* `tf.Session`: the runtime, runs the graph by running its operations (`tf.Session.run()`)\n",
    "* *operations* manipulate tensors\n",
    "* `tf.Tensor` the tensor object\n",
    "\n",
    "Graph and Session are the main componenst of TF Core.\n",
    "\n",
    "### Graph\n",
    "\n",
    "The graph uses the [dataflow programming model](https://en.wikipedia.org/wiki/Dataflow_programming), which models a program as a directed graph and operations make data flow in it. \n",
    "\n",
    "Note that a graph can be defined upfront and populated, or otherwise TF creates a default graph and it adds stuff to the more you define it. \n",
    "\n",
    "A graph can be run and attached operations using a `with` statement, as in `g = tf.Graph(); with g.as_default(): ...`\n",
    "\n",
    "### Tensors\n",
    "\n",
    "A fancy name for a multi-dimensional array. Tensors are handles for values that \"flow\" in the graph. A tensor has a given shape and relates to data of a given type. Data types include string, int, float, etc. A tensor's rank is its number of dimensions. A tensor is evaluated in a session.\n",
    "\n",
    "Tensors get a name, which is either the default coming from the operation that defines them (see later) or can be customised. \n",
    "\n",
    "#### Pre-existing tensors\n",
    "\n",
    "These are special classes for tensor that exist.\n",
    "\n",
    "* `tf.constant`: tensor containing constants, can be numbers or arrays but they don't get to change - correspond to a Const node in the graph\n",
    "* `tf.Variable`: a variable tensor, that is, it can change via operations in the graph. A variable tensor exists outside of the session it is run into\n",
    "* `tf.placeholder`: a placeholder acting like a promise to get some values later (see below for an example)\n",
    "* others (...)\n",
    "\n",
    "### Operations\n",
    "\n",
    "* Each operation in a graph has a name, this can be given when creating the operation or gets default value from the API\n",
    "\n",
    "### Layers\n",
    "\n",
    "Layers are a way to group together operations. In a neural net framework, they are the layers of the network. For instance, `tf.layers.Dense` is a for a fully connected layer in a net with activation , but it can be used more generally beyond a net, see [here](https://www.tensorflow.org/guide/low_level_intro#layers).\n",
    "\n",
    "### Devices\n",
    "\n",
    "Where to run the operations (for instance on CPU or GPU) can be chosen by using `tf.device`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with tensors\n",
    "\n",
    "Note that given [signature](https://www.tensorflow.org/api_docs/python/tf/constant), the name of a constant tensor defaults to \"Const\". If a name is not unique, TF appends \"\\_x\" to it for identification, x being the order of definition of that tensor. It also appends \":y\", y being the index of the tensor in the operation it comes from ([ref](https://stackoverflow.com/questions/36150834/how-does-tensorflow-name-tensors)).\n",
    "\n",
    "Tensors are named with the operation that defines/produces them. Note that calling things like `tf.constant` does call a `tf.Operation` which produces the tensor. Technically the name is given to the operation here, not to the tensor. \n",
    "\n",
    "In the little examples below, we're defining tensors and performing some operations; because no graph has been defined beforehand, they'll get added to the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Const:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'Const_1:0' shape=() dtype=int32>,\n",
       " tf.int32,\n",
       " tf.float32,\n",
       " tf.float32,\n",
       " 'Const_2:0',\n",
       " 'Const:0',\n",
       " 'e:0',\n",
       " 'f:0',\n",
       " <tf.Tensor 'Rank:0' shape=() dtype=int32>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define some tensors for numbers\n",
    "a = tf.constant(3, dtype=tf.int32)\n",
    "b = tf.constant(4)       # this will infer the type as int, if you put value \"4.0\" will infer type as float\n",
    "c = tf.constant(5, dtype=tf.float32)\n",
    "d = tf.constant(1.0)\n",
    "e = tf.constant([1, 2], name='e')\n",
    "f = tf.constant([[1, 2], [0, 1]], name='f')\n",
    "\n",
    "a, b, b.dtype, d.dtype, c.dtype, c.name, a.name, e.name, f.name, tf.rank(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tensors to perform simple numerical operations\n",
    "\n",
    "Have tensorflow do something you could easily do witout it. To evaluate the operation, and display its result, you need a session. Note that much like `tf.add`, there's many opeations available in the API. \n",
    "\n",
    "Note that the operations may accept stuff not formatted as a tensor in input (tensor-like input), which gets transformed into tensor under the hood. See [docs](https://www.tensorflow.org/guide/graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(tf.int32, TensorShape([]), <tf.Tensor 'Rank_1:0' shape=() dtype=int32>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summing them - returns a tensor\n",
    "tot = a + b\n",
    "\n",
    "type(tot)\n",
    "tot\n",
    "tot.dtype, tot.shape, tf.rank(tot)  # rank is 0 as it's a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the result - you need a session\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also add tensors like\n",
    "\n",
    "tot2 = tf.add(a, b, name='add_op')   # name the operation, will appear in the graph\n",
    "sess.run(tot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot3 = tf.add(1, 2)\n",
    "sess.run(tot3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use placeholders\n",
    "\n",
    "The following will be like defining a function that takes two args in input and returns the sum. It will be vealuated with a session which needs a `feed_dict` passed to know what the args are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will execute the operation, with the passed values for x and y\n",
    "\n",
    "sess.run(z, feed_dict={x: 1, y: 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and of course can do same with add instead\n",
    "s = tf.add(x, y)\n",
    "sess.run(s, feed_dict={x: 1, y: 2.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing how to use devices\n",
    "\n",
    "Note that TF will choose the GPU if available, but you can decide to ship some operations to the CPU instead. You can customise the choices over which device runs what of your graph.\n",
    "\n",
    "You can use multiple devices for computation, see [here](https://www.tensorflow.org/guide/using_gpu#using_multiple_gpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ship operations to the CPU\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "  z = x + y\n",
    "\n",
    "# Ship operations to the GPU\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "  z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed computation\n",
    "\n",
    "You can ship operations to different devices in a distributed setting. \"ps\" is the server node, \"worker\" are slave nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/job:ps/task:0\"):\n",
    "  z = x + y\n",
    "\n",
    "with tf.device(\"/job:ps/task:1\"):\n",
    "  z1 = x + y\n",
    "\n",
    "with tf.device(\"/job:worker\"):\n",
    "  z2 = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a small model\n",
    "\n",
    "You need to add each element one by one. This example is totally taken from the tutorial listed in the refs, it trains a linear regression. To do it we'll use\n",
    "\n",
    "* A `tf.layers.Dense` as the linear model as it has a linear activation per default (differnet activations can be set)\n",
    "\n",
    "We will create a graph for it rather than using the default one, it is run for TB below so can be visualised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.55902\n",
      "24.81181\n",
      "17.353725\n",
      "12.177895\n",
      "8.585681\n",
      "6.092307\n",
      "4.3614025\n",
      "3.1595626\n",
      "2.324834\n",
      "1.7448418\n",
      "1.3416101\n",
      "1.0610335\n",
      "0.86556923\n",
      "0.72916746\n",
      "0.6337526\n",
      "0.5667821\n",
      "0.5195532\n",
      "0.48602697\n",
      "0.4620135\n",
      "0.444605\n",
      "0.4317841\n",
      "0.42215088\n",
      "0.41473395\n",
      "0.4088591\n",
      "0.40405878\n",
      "0.40000832\n",
      "0.39648253\n",
      "0.39332497\n",
      "0.3904272\n",
      "0.387714\n",
      "0.38513297\n",
      "0.38264787\n",
      "0.38023347\n",
      "0.37787226\n",
      "0.37555212\n",
      "0.37326446\n",
      "0.3710034\n",
      "0.36876485\n",
      "0.36654592\n",
      "0.36434466\n",
      "0.3621595\n",
      "0.3599894\n",
      "0.35783386\n",
      "0.35569218\n",
      "0.3535639\n",
      "0.351449\n",
      "0.34934697\n",
      "0.3472577\n",
      "0.34518117\n",
      "0.34311706\n",
      "0.34106544\n",
      "0.3390262\n",
      "0.33699912\n",
      "0.33498418\n",
      "0.3329813\n",
      "0.33099043\n",
      "0.32901144\n",
      "0.32704437\n",
      "0.32508904\n",
      "0.32314533\n",
      "0.3212133\n",
      "0.3192928\n",
      "0.31738386\n",
      "0.31548625\n",
      "0.3136\n",
      "0.311725\n",
      "0.30986124\n",
      "0.30800864\n",
      "0.30616716\n",
      "0.30433658\n",
      "0.30251706\n",
      "0.30070835\n",
      "0.2989104\n",
      "0.2971233\n",
      "0.2953468\n",
      "0.293581\n",
      "0.29182574\n",
      "0.29008093\n",
      "0.2883466\n",
      "0.28662264\n",
      "0.28490892\n",
      "0.2832055\n",
      "0.28151226\n",
      "0.27982914\n",
      "0.2781561\n",
      "0.27649307\n",
      "0.2748399\n",
      "0.27319673\n",
      "0.2715633\n",
      "0.26993972\n",
      "0.26832575\n",
      "0.26672146\n",
      "0.2651268\n",
      "0.26354167\n",
      "0.26196596\n",
      "0.26039967\n",
      "0.2588428\n",
      "0.25729525\n",
      "0.2557569\n",
      "0.25422776\n",
      "[[-0.81170595]\n",
      " [-1.393327  ]\n",
      " [-1.9749482 ]\n",
      " [-2.5565693 ]]\n"
     ]
    }
   ],
   "source": [
    "# create the graph\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "\n",
    "    # x and y training data, as matrices of single-number arrays\n",
    "    x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)\n",
    "    y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)\n",
    "\n",
    "    # Using a Dense layer as the linear model\n",
    "    linear_model = tf.layers.Dense(units=1)\n",
    "\n",
    "    # Define the operation (set of as it's a layer) to run model for prediction\n",
    "    y_pred = linear_model(x)\n",
    "\n",
    "    # Define the loss as a MSE\n",
    "    loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "\n",
    "    # Define a GD as optimizer with chosen learning rate, and the operation to train\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    for i in range(100):\n",
    "      _, loss_value = sess.run((train, loss))\n",
    "      print(loss_value)\n",
    "\n",
    "    print(sess.run(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on sessions\n",
    "\n",
    "Sessions define the context in which the operation runs. In this sense, it can be run within a `with` statement (`with tf.Session() as sess: ...`), otherwise you need to call a `.close()` on it when done. It can be started locally or on remote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "Keras is a high-level API made to ascertain some logics above. It can be called from TF as `tf.keras` and you can use pretty much the same code you'd use in Keras. Note that Keras has a callback for TB so that can be used. \n",
    "\n",
    "Also note this very nice thing that you can run Keras jobs on multiple GPUs, see [docs](https://www.tensorflow.org/guide/keras#multiple_gpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators\n",
    "\n",
    "Estimators in TF are classed as high-level API. Estimators are full models that you can train and evaluate, then use for predictions. They're useful so all the logging and low-level preparation of the objects that go into building your models are handled for you. An estimator is callable as `tf.estimators.Estimator`, and there are some pre-made ones, like a linear regressor, which are sub-classes of this base class (find them under `tf.estimator`). Otherwise you can create your own custom ones, as instances of the base class.\n",
    "\n",
    "Using estimators entails:\n",
    "\n",
    "1. Creating input functions and choosing features to use\n",
    "2. Defining the estimator to use those features and its hyperparams\n",
    "3. Call methods on the estimator to perform operations\n",
    "\n",
    "Note that running a job via an estimator does not require you to write the summary, as it'll be written to a `model_dir` by the estimator itself on its own, but you can customise this as a kwarg of the Estimator. Also, it will use a default config (containing info how often to) unless you soecify one in param `config`. If you want to customise this, you need to pass your own `tf.estimator.RunConfig` instead.\n",
    "\n",
    "Note that a FileWriter is not needed with estimators (unless you need to set it yourself) as it's created automatically. It will merge and log metrics in TB every 100 steps default. The `model_dir` contains where stuff is stored for the model, and it is created when the estimator is istantiated.\n",
    "\n",
    "### Input functions\n",
    "\n",
    "Input functions are functions to create a `tf.data.Dataset` object to contain features (as dictionary) and labels (as array). An simple input function is for instance (for an IRIS dataset classification):\n",
    "\n",
    "```py\n",
    "def input_evaluation_set():\n",
    "    features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "                'SepalWidth':  np.array([2.8, 2.3]),\n",
    "                'PetalLength': np.array([5.6, 3.3]),\n",
    "                'PetalWidth':  np.array([2.2, 1.0])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels\n",
    "```\n",
    "\n",
    "But it is usually convenient to use the Dataset API\n",
    "\n",
    "### Datasets\n",
    "\n",
    "`tf.data.Dataset` is the API to parse data and creating sets for training and evaluating. It is convenient as it handles cases and formattings for you. For instance it is useul top fetch many data files into a single object.\n",
    "\n",
    "\n",
    "### Runconfig\n",
    "\n",
    "The config tells things like how often to log data for the summaries and stuff like. You can attach it to an estimator via the `config` parameter, and customise its params, like how often to record steps data.\n",
    "\n",
    "### Custom estimators\n",
    "\n",
    "In tensorflow/models, there are two examples that do the same thing (classification on the Iris dataset), except that [one](https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py) uses a pre-made estimator as a DNNClassifier, the [other](https://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py) builds it as custom.\n",
    "\n",
    "You need to build a custom estimator if the pre-existing ones are not good for your task, like you need to build a network in a specific way, or report your own custom metrics. Stuff like writing the input functions to sort the features stays the same between the two methods of estimators, pre-made or custom. On top of that, for custom estimatora you need to write a model function, which is what builds the actual model. A model function has signature \n",
    "\n",
    "```py\n",
    "def model_fn(features, labels, mode, params):\n",
    "```\n",
    "\n",
    "with the first two args from the input function, mode as an instance of `tf.estimator.ModeKeys` (tells if used for training, evaluating or inferring) and params containins further configuration.\n",
    "\n",
    "In all three modes the model function needs to return a `tf.estimator.EstimatorSpec`.\n",
    "\n",
    "* For training mode, you need to set an optimizer, from `tf.train`, the `EstimatorSpec` will contain the trainig step and the loss\n",
    "* For evaluation mode, the `EstimatorSpec` contains the loss and metrics - metrics are in `tf.metrics` and you can decide which ones to track, or craft custom ones\n",
    "* For prediction mode, the `EstimatorSpec` \n",
    "\n",
    "### TPU support\n",
    "\n",
    "Estimators with TPU support are in [here](https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimator), they have their own class inheriting from `Estimator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints\n",
    "\n",
    "Checkpoints are stored when training, you can customise how often. If the model is trained again, last checkpoint gets loaded.\n",
    "\n",
    "> TODO does this mean training resumes to where it was?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol buffers\n",
    "\n",
    "It's a Google way to serialise structured data. tensorflow/models contains some text files in protos, which when executed with `protoc object_detection/protos/*.proto --python_out=.` generate python code whose purpose is to do this serialisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "It's a UI to visualise learning. It has several tabs, to visualise the graph. It is under continuous development and several features are planned to be added.\n",
    "\n",
    "* `tf.summary.FileWriter` wirtes the data for TB to display.\n",
    "* There are multiple tabs, coming from the summary\n",
    "\n",
    "You can run TB after having created the summary, by running `tensorboard --logdir=<dir_name>` from the command line. The summary will create a file named \"events.out.tfevents.{timestamp}.{hostname}\" in the directory it is generated and this the path you need to give the tensorboard command. TB runs on port 6006 by default.\n",
    "\n",
    "### The summary\n",
    "\n",
    "It is a TF operation served to TB and is generated in multiple types, which correspond to different tabs in the UI:\n",
    "\n",
    "* scalar: displays the metrics of the model to track how it is performing while it trains/evaluates\n",
    "* image (visualises what the model is learning a set of images)\n",
    "* audio\n",
    "* histogram/distribution: charts here are made by a guy who was at the NYT earlier, give aggregated data\n",
    "* tensor: under dev - new thing \n",
    "\n",
    "A summary runs protocol buffers.\n",
    "\n",
    "### Displaying the graph\n",
    "\n",
    "* The graph can be structured in the code for best display - nodes can be given labels, otherwise you risk not understanding anything of it\n",
    "* Colour-coding is applied to tag elements that are of the same kind\n",
    "* Subgraphs are clustered together, you can click on them to expand\n",
    "\n",
    "### Hyperparams search\n",
    "\n",
    "* Can generate its own summary\n",
    "\n",
    "### Embedding viz\n",
    "\n",
    "Projects high-dim data to 3 dimensions, but needs to be called specifically. For instance allows to do vizs of data with PCA/t-SNE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a writer for a custom graph - using the g one above\n",
    "\n",
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Intro to the low-level API, from Tensorflow official](https://www.tensorflow.org/guide/low_level_intro)\n",
    "* Some parts on Tensorboard has been derived from the [Dev Summit 2017 video](https://www.tensorflow.org/guide/summaries_and_tensorboard) they have on the official TF page\n",
    "* On [tensors](https://www.tensorflow.org/guide/tensors), on [graphs and sessions](https://www.tensorflow.org/guide/graphs)\n",
    "* On using devices and [GPUs specifically](https://www.tensorflow.org/guide/using_gpu)\n",
    "* On the [Keras TF API](https://www.tensorflow.org/guide/keras)\n",
    "* On [TF Estimators](https://www.tensorflow.org/guide/premade_estimators) and especially on how to build a custom one, an [example](https://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py)\n",
    "* Walkthrough of the MNIST [example](https://jhui.github.io/2017/03/12/TensorBoard-visualize-your-learning/)\n",
    "* On [Protocol Buffers](https://developers.google.com/protocol-buffers/?hl=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
