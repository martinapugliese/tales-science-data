{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is\n",
    "\n",
    "Apache Spark is a general purpose open source distributed system for cluster computing  on large datasets. It is natively written in Scala but has API's for other languages. It was originally developed at the University  of California at Berkeley in 2009 and then donated to the [Apache Foundation](https://www.apache.org). \n",
    "\n",
    "<img src=\"../../imgs/spark.pdf\" width=\"600\" />\n",
    "\n",
    "The figure above illustrates in a very rough way what the Apache Spark environment consists of. Spark ML and MLlib are the libraries for Machine Learning: the first, newer, works with DataFrames; the second with RDDs (*Resilient Distributed Datasets*, see below). API's exist in Scala (Spark native language), Java, R and Python.\n",
    "\n",
    "The concepts illustrated briefly here in this note are reworked from the [[1]](#1), which is full of great material, including tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Data Model: RDDs and DataFrames\n",
    "\n",
    "The core of the  computation in Spark lies in the use of RDDs (*Resilient Distributed Datasets*). An RDD is a partitioned collection of elements that can be operated in parallel; it is recomputed on node failures  and is created by the so-called *SparkContext* from input sources (local file system, [the HDFS](hadoop.ipynb#The-HDFS), ...)\n",
    "\n",
    "RDDs have these features:\n",
    "\n",
    "* are immutable once constructed;\n",
    "* enable operators to run in parallel;\n",
    "* are able to track info to recompute potential lost data;\n",
    "* the more the partitions, the more parallelism\n",
    "* are lazily evaluated\n",
    "\n",
    "A (distributed) DataFrame can be built from an RDD and is conceptually  similar to the corresponding R and Pandas ones.\n",
    "\n",
    "The fact that RDDs are lazily evaluated means that no operation is actually executed until an *action* is called, and this allows to skip intermediate big results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spark programming model\n",
    "\n",
    "The *SparkContext* tells Spark how and where to access the cluster and is used to create the RDDs. Its *master* parameter sets the type and size of the cluster:\n",
    "\n",
    "* *local*: run locally with a single worker\n",
    "* *local[4]*: run locally with 4 workers (ideally to be set to the number of cores)\n",
    "* *spark://host:port*: run on an external specified cluster\n",
    "\n",
    "<img src=\"../../imgs/spark-model.pdf\" width=\"600\" />\n",
    "\n",
    "The figure above illustrates the relation between drivers and workers in Spark, FS is the file system.\n",
    "\n",
    "### Transformations and Actions\n",
    "\n",
    "<img src=\"../../imgs/spark-operations.pdf\" width=\"400\" align=\"left\" style=\"margin:0px 50px\"/>\n",
    "\n",
    "*Transformations* are meant to transform the RDD, creating a new dataset, are lazily applied, and the execution takes place only when an *action* is called. Actions cause all transformations to be executed.\n",
    "\n",
    "Examples of transformation are *map*, *filter*, *distinct*;  examples of actions are *reduce*, *take, *collect*.\n",
    "\n",
    "The Spark programming model follows these steps:\n",
    "\n",
    "* Create RDD from data by parallelisation\n",
    "* Transform into a new RDD\n",
    "* Cache some RDDs for reuse (\\textit{persist})\n",
    "* Use action to execute parallel computation and produce results\n",
    "\n",
    "The problem with large global variables is that it is inefficient to send large data to each worker. To solve this, Spark has shared variables: *broadcasts* (send large, read-only values to the workers who save them) and *accumulators* (aggregate the values from the workers back to the driver and are write-only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Hadoop\n",
    "\n",
    "Generally speaking, Spark is faster: the use of memory guarantees it is roughly $~O(10^2)$ faster. The use of disk in Hadoop is slow for complex jobs and interactive queries. Spark leverages the idea of keeping more data into memory.\n",
    "\n",
    "|                      | Hadoop        | Spark          |\n",
    "| -------------------- |:-------------:| --------------:|\n",
    "| **Storage**          | Disk-only     | Disk or Memory |\n",
    "| **Operations**       | Map & Reduce      | Map, Reduce, Join , Sample, ... |\n",
    "| **Execution model**  | Batch      | Batch, interactive, streaming |\n",
    "| **Programming**      | Java      | Java, Scala, R, Python |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "1. <a name=\"docs\"></a> [The Apache Spark docs](http://spark.apache.org/documentation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
